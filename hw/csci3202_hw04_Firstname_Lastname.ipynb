{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top'></a>\n",
    "\n",
    "# CSCI 3202, Spring 2018\n",
    "# Assignment 4\n",
    "# Due:  Wednesday 21 March 2018 by 12:00 PM\n",
    "\n",
    "<br>\n",
    "\n",
    "### Your name:\n",
    "\n",
    "<br>\n",
    "\n",
    "**Note:** Some packages to load, helper functions and unit tests are defined at [the bottom of this notebook](#helpers)\n",
    "\n",
    "Shortcuts:  [top](#top) || [1](#p1) | [1a](#p1a) | [1b](#p1b) | [1c](#p1c) | [1d](#p1d) | [1e](#p1e) | [1f](#p1f) | [1g](#p1g) | [1h](#p1h) || [2](#p2) | [2a](#p2a) | [2b](#p2b) | [2c](#p2c) | [2d](#p2d) | [2e](#p2e) || [3](#p3) | [3a](#p3a) | [3b](#p3b) | [3c](#p3c) | [3d](#p3d) | [3e](#p3e) || [helpers](#helpers)\n",
    "\n",
    "**Before you begin:** You will also need to download a few data sets from the Piazza page, under the Resources tab.\n",
    "1. data_sealevel.csv\n",
    "1. data_temperature.csv\n",
    "1. data_socks.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='p1'></a>[Back to top](#top)\n",
    "\n",
    "## Problem 1:  Calibrating a model for global mean sea level changes\n",
    "\n",
    "<img src=\"http://www.anthropocenemagazine.org/wp-content/uploads/2017/05/future-sea-levels.jpg\" width=\"250\">\n",
    "\n",
    "<a id='p1a'></a>\n",
    "\n",
    "### (1a) Load and plot some data.\n",
    "\n",
    "Let's load a couple data sets.  One is a data set of global mean sea levels, and the other is a data set of global mean temperatures:\n",
    "* `sealevel` will be a list of global mean sea levels (millimeters)\n",
    "* `sealevel_sigma` will be a list of the *uncertainty* in global mean sea levels (millimeters), and\n",
    "* `temperature` will be a list of global mean temperatures (degrees Celsius).\n",
    "\n",
    "Note that depending on where you save these CSV data files, you may need to change the file names below in the `pd.read_csv` function calls to reflect the proper paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "year = []\n",
    "sealevel = []\n",
    "sealevel_sigma = []\n",
    "temperature = []\n",
    "\n",
    "dfSealevel = pd.read_csv(\"data_sealevel.csv\")\n",
    "dfTemperature = pd.read_csv(\"data_temperature.csv\")\n",
    "\n",
    "# We aren't doing any heavy-duty stats stuff, so let's just keep what we need as regular lists\n",
    "year = dfSealevel[\"year\"].tolist()\n",
    "sealevel = dfSealevel[\"sealevel\"].tolist()\n",
    "sealevel_sigma = dfSealevel[\"uncertainty\"].tolist()\n",
    "temperature = dfTemperature[\"temperature\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check** to make sure (1) temperature, year and sea level time series are all the same length and period; (2) data need to be properly *normalized* against some reference period. In climate science, often the years 1961-1990 are used as the reference period.  So conduct a check to make sure that the mean sea level and mean global mean temperature for the 1961-1990 (inclusive) period are both 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data points as a scatter plots, and plot them side-by-side-by-side (one row, three columns of figures). The point here is learn how to customize your figures a bit more, and also because computer screens are (typically) wider than they are tall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How does the uncertainty in global mean sea levels change as a function of time?  When is the uncertainty the highest?  Give one reason why you think this might be the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<a id='p1b'></a>\n",
    "\n",
    "### (1b)  The \"out-of-box\" sea-level model\n",
    "\n",
    "In your plot from **(1a)**, you should see quite an apparent relationship between increasing temperatures and rising sea levels.  Seeems like someone should try to model the relationship between those two, huh?\n",
    "\n",
    "In the [helper functions below](#helpers), a simple model for temperature-driven changes in global mean sea level (GMSL) is defined. This is the model of [Rahmstorf (2007)](http://science.sciencemag.org/content/315/5810/368).\n",
    "\n",
    "The `slr` model takes two parameters, $\\alpha$ and $T_{eq}$, and requires a time series of global mean temperatures: `slr(alpha, Teq, temperature)`.\n",
    "* `alpha` is the sensitivity of sea-level changes to changes in global temperature. The units for $\\alpha$ are millimeters of sea-level changes per year, or mm y$^{-1}$.\n",
    "* `Teq` is the equilibrium global mean temperature, with units of degrees Celsius.\n",
    "* `temperature` is the time series of global mean surface temperatures, assumed to be relative to the 1961-1990 mean. We read a temperature data set above, which we will use for this input.\n",
    "\n",
    "For now, you do not need to worry too much about how this model works.  It is very simple, and widely used, but the point here is that you can plug in a particular set of temperatures (the model **forcing**) and parameters ($\\alpha$ and $T_{eq}$), and out pops a time series of simulated global mean sea levels.\n",
    "\n",
    "**Our goal:**  pick good values for $\\alpha$ and $T_{eq}$, so that when we run the `slr` model using the observations of temperature (which we plotted above), the model output matches well the observations of global mean sea level (which we also plotted above).\n",
    "\n",
    "The whole process of figuring out what these good parameter values are is called **model calibration**, and it's awesome.  Let's have a look at why we need to do any calibration in the first place, shall we?\n",
    "\n",
    "The default parameter choices given in the Rahmstorf (2007) paper are $\\alpha=3.4$ mm y$^{-1}$ and $T_{eq} = -0.5\\ ^{\\circ}$C.\n",
    "\n",
    "Make a plot that contains:\n",
    "* the observed sea level data as scatter points\n",
    "* the modeled sea levels as a line, using the temperature observations from above as the `temperature` input\n",
    "* an appropriate legend and axis labels\n",
    "* $x$ axis is years\n",
    "* $y$ axis is sea level\n",
    "\n",
    "Note that after you run the `slr` model, you will need to **normalize** the output relative to the 1961-1990 reference period.  That is because you are going to compare it against data that is also normalized against this reference period. The `years` that correspond to the model output should be the same as the `years` that correspond to the `temperature` input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your plot above ought to show decent match for the late 1900s, but diverge a bit further back in time.\n",
    "\n",
    "**The point:**  We can do better than this \"out-of-the-box\" version of the Rahmstorf sea level model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<a/ id='p1c'></a>\n",
    "\n",
    "### (1c)   Figuring out our objective function\n",
    "\n",
    "As our **objective function**, we will use the joint likelihood function of the observed sea level data, given the model simulation.  For a single data point $y_i$ in year $i$, with associated uncertainty $\\sigma_i$, we can assume the likelihood for our model simulation in year $i$, $\\eta_i$, follows a normal distribution centered at the data point ($y_i$) and with standard deviation given by the associated uncertainty ($\\sigma_i$).  The model simulation is a **deterministic** result of our parameter choices $\\alpha$ and $T_{eq}$, so we write the likelihood as:\n",
    "\n",
    "$$L(y_i \\mid \\alpha, T_{eq}) = \\dfrac{1}{\\sqrt{2 \\pi} \\sigma_i} e^{-\\dfrac{(\\eta_i(\\alpha, T_{eq}) - y_i)^2}{2\\sigma_i^2}}$$\n",
    "\n",
    "But that only uses a single data point.  Let's use all the data!  The **joint likelihood** is the product of all of the likelihoods associated with the individual data points.\n",
    "\n",
    "$$L(\\mathbf{y} \\mid \\alpha, T_{eq}) = \\prod_{i=1}^N L(y_i \\mid \\alpha, T_{eq})$$\n",
    "\n",
    "However, this joint likelihood is the product of a lot of numbers that are less than 1, so it will be **tiny**.  Instead, we should try to optimize the **joint log-likelihood**, which is simply the (natural) logarithm of the joint likelihood function.\n",
    "\n",
    "$$l(\\mathbf{y} \\mid \\alpha, T_{eq}) = \\log{(L(y \\mid \\alpha, T_{eq}))}$$\n",
    "\n",
    "**Show** (using math) that if we assume the observational data ($y_i$) are all independent, then the joint log-likelihood is:\n",
    "\n",
    "$$l(\\mathbf{y} \\mid \\alpha, T_{eq}) =  -\\dfrac{N}{2} \\log{(2\\pi)} - \\sum_{i=1}^N \\log{(\\sigma_i)} - \\dfrac{1}{2}\\sum_{i=1}^N \\left( \\dfrac{\\eta_i(\\alpha, T_{eq}) - y_i}{\\sigma_i} \\right)^2$$\n",
    "\n",
    "where, $\\mathbf{y} = [y_1, y_2, \\ldots, y_N]$ is the entire vector (list) of sea level observations, $\\eta(\\alpha, T_{eq}) = [\\eta_1(\\alpha, T_{eq}), \\eta_2(\\alpha, T_{eq}), \\ldots, \\eta_N(\\alpha, T_{eq})]$ is the entire vector (list) of `slr` model output when the parameter values $\\alpha$ and $T_{eq}$ are used, and $N$ is the number of observations we have.\n",
    "\n",
    "**Note:** Yes, I'm mixing the typical indexing of $1-N$ with what you'll need to implement, which is the Pythonic $0-(N-1)$. Keep this in mind as you code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Show using nice LateX/Markdown formatting..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<a/ id='p1d'></a>\n",
    "\n",
    "### (1d)   Defining our objective function\n",
    "\n",
    "Now define a `log_likelihood(parameters, obs_temp, obs_mu, obs_sigma)` function:\n",
    "* `parameters`: argument that is a list of two parameter values, $[\\alpha, T_{eq}]$\n",
    "  * within the likelihood function, you will need to generate the model simulation $\\eta(\\alpha, T_{eq})$ using the input `parameters`, for comparison against the observational data\n",
    "* `obs_temp`: argument that is a time series (list) of observed global mean temperatures, that will be used to run the `slr` model. Provide a default value of `temperature` for this, because we only have one temperature data set to use, and we don't want to keep typing out inputs for this.\n",
    "* `obs_mu`: argument that is a time series (list) of observed values, that will be used for comparison against the `model` output. Provide a default value of `sealevel` here, again because we won't be changing the observational data.\n",
    "* `obs_sigma`: argument that is a time series (list) of the corresponding uncertainties in the observational data. Simiarly, provide a default value of `sealevel_sigma` here.\n",
    "* all three of these `obs_*` inputs should be lists, and should be the same length\n",
    "* this routine should return a **single** float number, that is the joint log-likelihood of the given `model` simulation.\n",
    "\n",
    "You may find the `scipy.stats` package to be useful here, or you can calculate this straight from the equation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<a/ id='p1e'></a>\n",
    "\n",
    "### (1e)  Playing with our shiny new objective function!\n",
    "\n",
    "Armed with our **log-likelihood** objective function, let's see what it looks like for various values of $\\alpha$, while keeping $T_{eq}$ fixed at its default value ($T_{eq}=-0.5$). Try a variety of $\\alpha$ values. Use a wide enough range that you can tell what the behavior of the $\\alpha$-log-likelihood relationship is from a plot of the two (below).\n",
    "\n",
    "Then, make a plot of the resulting log-likelihood values, as a function of the values for the $\\alpha$ parameter used. Label your axes appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now explore a range of values around the default of $T_{eq}= -0.5 ^{\\circ}$C, while keeping the $\\alpha$ parameter constant at its default of 3.4 mm y$^{-1}$. Generate a plot of some test values of $T_{eq}$ against the resulting log-likelihood function values.  Label your axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection 1:** Write a sentence or two commenting on these two plots.  What do you think are the best values for $\\alpha$ and $T_{eq}$ (so far)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection 2:** How *likely* do values of $\\alpha < 0$  or $T_{eq} > 0$ appear to be?  If these values are implausible, what does that mean in terms of the sea level-temperature system that we are modeling?  You should talk about the actual physical system, not just vague statements like \"$\\alpha < 0$ looks improbable\".  While that statement is true, it is not very insightful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection 3:** Based on your plots above, do you think one of hill-climbing or simulated annealing will work better than the other to find the global maximum in the objective function, or does it appear that either will find the optimum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<a/ id='p1f'></a>\n",
    "\n",
    "### (1f)  Defining our class structure\n",
    "\n",
    "That exploration was a nice way to get a sense of how the $\\alpha$ and $T_{eq}$ parameters affect the quality of the model output, but it was not particularly rigorous. Let's apply our **hill-climbing** algorithm to this problem.\n",
    "\n",
    "Using our in-class activity as a guide, do the following:\n",
    "\n",
    "* Define a `State` class, with attributes for the parameter values (which define the state) and the objective function value of that state.\n",
    "* Define a `Problem_hillclimb` **sub-class** of the more general class `Problem`, with:\n",
    "  * attributes for the current `State` (a `State` object), the `objective_function` (the log-likelihood defined above), and `stepsize`. You will need to play around to decide what an appropriate stepsize is. Keep in mind that you may need a different stepsize for each of $\\alpha$ and $T_{eq}$.\n",
    "  * methods for `moves` (return the list of all possible moves from the current state) and `best_move` (return the move that maximizes the objective function).\n",
    "  * the `moves` available should be in proper 2-dimensional space.  Do **not** simply optimize one parameter, keeping the other fixed, then optimize the other parameter, while keeping the first fixed.  (*That method *can* work, but there are some theoretical issues that would need to be tackled, and we are not getting into that here.*) You are allowed to restrict yourself to movements along a grid, as long as you entertain steps in both the $\\alpha$ and the $T_{eq}$ directions.\n",
    "  * The `Problem` class should have everything in it that you might need to apply different optimization algorithms, while the `Problem_hillclimb` sub-class will have the methods and attributes specific to hill-climbing defined.\n",
    "* Define the `hill_climb` algorithm, with any necessary modifications (here, and in the above classes) for the new 2-dimensional state space.\n",
    "  * `hill_climb(problem, n_iter)`:  arguments are a `Problem_hillclimb` object and number of iterations, `n_iter`\n",
    "  * return a `State` that corresponds to the algorithm's guess at a global maximum\n",
    "\n",
    "Subject to the above constraints, you may implement these however you would like. **If you do not use a sub-class (properly), you will lose points.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now:**\n",
    "1. define an initial state object, using the default values from Rahmstorf 2007 as a starting point.\n",
    "2. define a hill-climbing problem object, using this initial state, the joing log-likelihood objective function, and stepsize(s) of your choosing. (The stepsize(s) may require some playing around to find something you are happy with.)\n",
    "3. ***hill-climb!!!*** Use a number of iterations that you deem appropriate. \n",
    "\n",
    "Play around until you have a simulation that you are happy with.  Then:\n",
    "1. Print to screen the parameter values and corresponding log-likelihood value.\n",
    "2. Compare this calibrated log-likelihood value to the \"out-of-box\" model (above).\n",
    "3. Make a plot of:\n",
    "  * the sea level observations as scatter points\n",
    "  * the uncalibrated model as one line\n",
    "  * the calibrated model as another line (using a different color!)\n",
    "  * include axis labels and a legend\n",
    "  \n",
    "**\"Unit tests\":**\n",
    "* As a benchmark, make sure that your log-likelihood is *at least* -500.\n",
    "* Your calibrated (optimized) model simulation should be going pretty much straight through the data points.\n",
    "* Remember to normalize your model against the 1961-1990 reference period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<a/ id='p1g'></a>\n",
    "\n",
    "### (1g)  Simulated annealing\n",
    "\n",
    "Let's re-calibrate the `slr` model. This time, we will use **simulated annealing**. Again, using our in-class activity as a guide, do the following:\n",
    "\n",
    "* Continue to use your `State` class above.\n",
    "* Define a `Problem_annealing` sub-class of the `Problem` class, with:\n",
    "  * attributes for the current `State` (a `State` object), the `objective_function` (the log-likelihood defined above), and `stepsize`. You will need to play around to decide what an appropriate stepsize is. Keep in mind that you may need a different stepsize for each of $\\alpha$ and $T_{eq}$.\n",
    "  * a method for `random_move`, to pick a random move **by drawing from a multivariate normal distribution**.  You should use the `stepsize` attribute as the covariance (width) for this.\n",
    "* Define the `simulated_annealing` algorithm, with any necessary modifications (here, and in the above classes) for the new 2-dimensional state space.\n",
    "  * `simulated_annealing(problem, n_iter)`:  arguments are a `Problem_annealing` object and number of iterations, `n_iter`\n",
    "  * return a `State` that corresponds to the algorithm's guess at a global maximum, and corresponding objective function value\n",
    "\n",
    "Subject to the above constraints, you may implement these however you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now:**\n",
    "1. define an initial state object, using the default values from Rahmstorf 2007 as a starting point.\n",
    "2. define a simulated annealing problem object, using this initial state, the log-likelihood objective function, an appropriate temperature updating schedule and stepsize(s) of your choosing. (The stepsize(s) may require some playing around to find something you are happy with.)\n",
    "  * note that this \"temperature\" is distinct from the actual physical temperature used as input to drive the `slr` model\n",
    "3. ***anneal!!!*** Use a number of iterations that you deem appropriate. \n",
    "\n",
    "Play around until you have a simulation that you are happy with.  Then:\n",
    "1. Print to screen the parameter values and corresponding log-likelihood value.\n",
    "2. Compare this calibrated log-likelihood value to the \"out-of-box\" model (above).\n",
    "3. Make a plot of:\n",
    "  * the sea level observations as scatter points\n",
    "  * the uncalibrated model as one line\n",
    "  * the calibrated model as another line (using a different color!)\n",
    "  * include axis labels and a legend\n",
    "  \n",
    "**\"Unit tests\":**  How does your model look when you plot it against the data? If it doesn't look good, then you failed this unit test :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<a/ id='p1h'></a>\n",
    "\n",
    "### (1h)  Wrap-up\n",
    "\n",
    "Consider the values you would choose for $\\alpha$ and $T_{eq}$ by only considering their **marginal** effects on the log-likelihood objective function (i.e., your answer to **1e**).  How do those compare to the values for $\\alpha$ and $T_{eq}$ that you found above using simulated annealing?  What do you think accounts for the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "<a id='p2'></a>[Back to top](#top)\n",
    "\n",
    "## Problem 2:  Playing \"intelligent\" Tic-Tac-Toe\n",
    "\n",
    "<img src=\"https://www.cookieshq.co.uk/images/2016/06/01/tic-tac-toe.png\" width=\"150\"/>\n",
    "\n",
    "<a id='p2a'></a>\n",
    "\n",
    "### (2a)   Defining the Tic-Tac-Toe class structure\n",
    "\n",
    "Fill in this class structure for Tic-Tac-Toe using what we did during class as a guide.\n",
    "* `moves` is a list of tuples to represent which moves are available. Recall that we are using matrix notation for this, where the upper-left corner of the board, for example, is represented at (1,1).\n",
    "* `result(self, move, state)` returns a ***hypothetical*** resulting `State` object if `move` is made when the game is in the current `state`\n",
    "* `compute_utility(self, move, state)` calculates the utility of `state` that would result if `move` is made when the game is in the current `state`. This is where you want to check to see if anyone has gotten `nwin` in a row\n",
    "* `game_over(self, state)` - this wasn't a method, but it should be - it's a piece of code we need to execute repeatedly and giving it a name makes clear what task the piece of code performs. Returns `True` if the game in the given `state` has reached a terminal state, and `False` otherwise.\n",
    "* `utility(self, state, player)` also wasn't a method earlier, but also should be.  Returns the utility of the current state if the player is X and $-1 \\cdot$ utility if the player is O.\n",
    "* `display(self)` is a method to display the current game `state`, You get it for free! because this would be super frustrating without it.\n",
    "* `play_game(self, player1, player2)` returns an integer that is the utility of the outcome of the game (+1 if X wins, 0 if draw, -1 if O wins). `player1` and `player2` are functional arguments that we will deal with in parts **2b** and **2d**.\n",
    "\n",
    "Some notes:\n",
    "* Assume X always goes first.\n",
    "* Do **not** hard-code for 3x3 boards.\n",
    "* You may add attributes and methods to these classes as needed for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class State:\n",
    "\n",
    "    # your code goes here\n",
    "    \n",
    "        \n",
    "class TicTacToe:\n",
    "    \n",
    "    def __init__(self, # your code goes here )\n",
    "                 \n",
    "    # ... and here\n",
    "                 \n",
    "         \n",
    "                 \n",
    "    def result(self, move, state):\n",
    "        '''\n",
    "        What is the hypothetical result of move `move` in state `state` ?\n",
    "        move  = (row, col) tuple where player will put their mark (X or O)\n",
    "        state = a `State` object, to represent whose turn it is and form\n",
    "                the basis for generating a **hypothetical** updated state\n",
    "                that will result from making the given `move`\n",
    "        '''\n",
    "\n",
    "        # your code goes here\n",
    "        \n",
    "\n",
    "        \n",
    "    def compute_utility(self, move, state):\n",
    "        '''\n",
    "        What is the utility of making move `move` in state `state`?\n",
    "        If 'X' wins with this move, return 1;\n",
    "        if 'O' wins return -1;\n",
    "        else return 0.\n",
    "        '''        \n",
    "\n",
    "        # your code goes here\n",
    "\n",
    "        \n",
    "\n",
    "    def game_over(self, state):\n",
    "        '''\n",
    "        Is the game over?  Return True/False.\n",
    "        The game is over if someone has won (utility!=0)\n",
    "        or there are no more moves left.\n",
    "        '''\n",
    "\n",
    "        # your code goes here\n",
    "        \n",
    "\n",
    "    \n",
    "    def utility(self, state, player):\n",
    "        '''\n",
    "        Return the value to the given player; 1 for win, -1 for loss, 0 otherwise.\n",
    "        '''\n",
    "\n",
    "        # your code goes here\n",
    "        \n",
    "        \n",
    "        \n",
    "    def display(self):\n",
    "        '''Display the current game state.'''\n",
    "        for row in range(1, self.nrow+1):\n",
    "            for col in range(1, self.ncol+1):\n",
    "                print(self.state.board.get((row, col), '.'), end=' ')\n",
    "            print()\n",
    "\n",
    "                 \n",
    "    def play_game(self, player1, player2):\n",
    "        '''\n",
    "        Play a game of tic-tac-toe!\n",
    "        player1 and player2 are function names, either `random_player`\n",
    "        (see 2b) or `alphabeta_player (see 2d)\n",
    "        '''\n",
    "\n",
    "        # your code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<a/ id='p2b'></a>\n",
    "\n",
    "### (2b) Define a random player\n",
    "\n",
    "Define a function `random_player` that takes a single argument of the `TicTacToe` class and returns a random move out of the available legal moves in the `state` of the `TicTacToe` game.\n",
    "\n",
    "In your code for the `play_game` method above, make sure that `random_player` could be a viable input for the `player1` and/or `player2` arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We know from experience and/or because I'm telling you right now that if two `random_player`s play many games of Tic-Tac-Toe against one another, whoever goes first will win about 58% of the time.  **Test your codes and verify** that this is the case by playing at least 1,000 games between two random players. **Report** the proportion of the games that the first player has won, lost, and came to a draw.\n",
    "\n",
    "**\"Unit tests\":** If you are wondering how close is close enough to 58%, I simulated 100 tournaments of 1,000 games each. The min-max range of win percentage by the first player was 54-63%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<a/ id='p2c'></a>\n",
    "\n",
    "### (2c) What about playing randomly on different-sized boards?\n",
    "\n",
    "What does the long-term win percentage appear to be for the first player in a 4x4 Tic-Tac-Toe tournament, where 4 marks must be connected for a win?  Support your answer using a simulation and printed output, similar to **2b**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Also:** The win percentage should have changed substantially. Did the decrease in wins turn into more losses for the first player or more draws? Write a few sentences explaining the behavior you observed.  *Hint: think about how the size of the state space has changed.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<a/ id='p2d'></a>\n",
    "\n",
    "### (2d) Define an alpha-beta player\n",
    "\n",
    "Alright. Let's finally get serious about our Tic-Tac-Toe game.  No more fooling around!\n",
    "\n",
    "Craft a function called `alphabeta_player` that takes a single argument of a `TicTacToe` class object and returns the minimax move in the `state` of the `TicTacToe` game. As the name implies, this player should be implementing alpha-beta pruning as described in the textbook and lecture.\n",
    "\n",
    "Note that your alpha-beta search for the minimax move should include function definitions and alternating recursive calls for `max_value` and `min_value` (see the aggressively realistic pseudocode from the lecture slides).\n",
    "\n",
    "In your code for the `play_game` method above, make sure that `alphabeta_player` could be a viable input for the `player1` and/or `player2` arguments. Because your `alphabeta_player` is about to mop the floor with your `random_player`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that your alpha-beta player code is working appropriately through the following tests, using a standard 3x3 Tic-Tac-Toe board. Run **only 30 games for each test**, and report for each test the number of wins, draws and losses for the first player.\n",
    "\n",
    "1. An alpha-beta player who plays first should never lose to a random player who plays second.\n",
    "2. A random player who plays first should never win to an alpha-beta player who plays second.\n",
    "3. Two alpha-beta players should always draw, and should always end up in the same terminal state.\n",
    "\n",
    "**Nota bene:** Test your code with fewer games between the players to start, because the alpha-beta player will require substantially more compute time than the random player.  This is why I only ask for 30 games, which still might take a couple minutes, depending on your machine. (FYI: 30 games between 2 alpha-beta players on my 2017 MacBook Pro takes about 1 minute.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<a/ id='p2e'></a>\n",
    "\n",
    "### (2e) What has pruning ever done for us?\n",
    "\n",
    "Calculate the number of number of states expanded by the minimax algorithm, with and without pruning, to determine the minimax decision from the initial empty 3x3 Tic-Tac-Toe board state.  This can be done in many ways, but writing out all the states by hand is **not** one of them (as you will find out!).\n",
    "\n",
    "Write a sentence or two, commenting on the difference in number of nodes expanded by each search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "<a id='p3'></a>[Back to top](#top)\n",
    "\n",
    "## Problem 3:  Bayesian sock estimation, rebooted\n",
    "\n",
    "<img src=\"https://pbs.twimg.com/media/B0Lbm4TCcAAMH26.jpg\" width=\"200\"/>\n",
    "\n",
    "Let's modify what we did in class (Monday 5 March) so we can estimate the posterior distribution for the number of socks an arbitrary member of our class has, ***given*** the data of unique/paired socks that come out of the dryer.\n",
    "\n",
    "**The point:** Yes, we know how many socks **each individual** has (because you did the Quizlet, right?). But what we do *not* have is a formal framework in which we can estimate the number of socks ***an arbitrary*** person has (a person selected at random).  Furthermore, we do our laundry multiple times, so we can update our prior beliefs multiple times too, and obtain better estimates of how many socks a mysterious individual has.\n",
    "\n",
    "**Applications:** Pollsters may know which party *a specific individual* will vote for in an election, but a **LOT** of money is invested in figuring out which party ***an arbitrary*** individual from a community will vote for, given some data.\n",
    "\n",
    "<br>\n",
    "\n",
    "<a id='p3a'></a>\n",
    "\n",
    "### (3a)   Read and plot our class data\n",
    "\n",
    "\n",
    "1. Read in the data set from Quizlet 8, `data_socks.csv`.  Feel free to steal liberally from the Problem 1 codes.\n",
    "1. Make a figure with two panels:\n",
    "  1. A histogram of the CSCI 3202 results for `n_socks`, superimposed with a histogram of some samples from the original negative binomial prior distribution from class using the Karl Broman/blog post parameters.\n",
    "  1. A histogram of the CSCI 3202 results for `prop_pairs`, superimposed with a histogram of samples from the original beta prior distribution from class.\n",
    "  \n",
    "Clearly label all axes and include a legend to distinguish between our class results and the previous prior distributions.\n",
    "\n",
    "How you present these distributions is up to you.  Be sure the histograms sharing axes are scaled so they both show up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<a/ id='p3b'></a>\n",
    "\n",
    "### (3b)  Revising our prior distributions\n",
    "\n",
    "Decide on new parameters for our two prior distributions, to fit the data from our class.  The way we are formalizing the problem here is also a bit different (total socks, as opposed to just the ones in the laundry, for example), so we would need to adjust anyhow.  You can also decide to use completely new prior distribution forms altogether if you would like.\n",
    "\n",
    "Clearly state what your new prior distribution parameters are and how you chose them.  Show any relevant calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the same two-panel figure as in **3a**, but instead of the Karl Broman/blog post prior histograms, include with our CSCI 3202 data your revised prior distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<a/ id='p3c'></a>\n",
    "\n",
    "### (3c) Do your laundry!\n",
    "\n",
    "**Now:** Suppose we draw 10 socks out of the laundry and there is one pair and 8 unique socks. Update your prior distributions based on these data.  Use the same **accept/reject likelihood function** form that we used in class. Use at least 10,000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide a two-panel figure (two rows x one column):\n",
    "1. Top panel is a histogram of samples from your prior distribution for `n_socks`\n",
    "1. Bottom panel is a histogram of samples from the posterior distribution resulting from your accept/reject approximate Bayesian computation\n",
    "\n",
    "Make sure the limits of your horizontal `n_socks`-axis are the same for both panels, to make for an easier comparison, and that all axes are labelled appropriately.\n",
    "\n",
    "Note that the model `sock_model` from class is defined in the helper functions below.  It should not need modified, but if you want to modify it you can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a few sentences commenting on the effect of the Bayesian update of our prior distribution.  (A \"Bayesian update\" simply refers to the part where we calculate the likelihood function and use it to modify our prior distribution into our posterior distribution.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the samples from the prior distributions as lists or in a Pandas data frame. You will use them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<a/ id='p3d'></a>\n",
    "\n",
    "### (3d) Do your laundry... again!\n",
    "\n",
    "Now treat your resulting *posterior distributions* as the sample data from our class to develop ***updated prior distributions***.  The easiest way to do this is to assume the same functional form, but update the parameters of the distribution.\n",
    "\n",
    "Clearly state what your updated prior distributions are and how you decided on these new distributions/parameters. Show any relevant calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you do your laundry a second time, again with all of your socks in there at once.  This time, you pull 12 socks out of the laundry and get 3 pairs and 6 distinct socks.\n",
    "\n",
    "**Now:** Based on our *updated* prior distributions, use the accept/reject likelihood function with these new data to update your priors a second time. Again, use at least 10,000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide a three-panel figure (three rows x one column):\n",
    "1. First two panels are the same as in **3c**\n",
    "1. Third (bottom) panel is the posterior distribution resulting from the second update\n",
    "\n",
    "So the top panel is only the prior distribution (no data), the middle panel is a posterior updated once with data and the bottom panel is a posterior distribution updated with two sets of data.\n",
    "\n",
    "Each update with data is called an **assimilation step** - we are assimilating laundry data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<a/ id='p3e'></a>\n",
    "\n",
    "### (3e) Some calculations\n",
    "\n",
    "Calculate the 5-95% percentile (quantile) range for `n_socks` using each of: (1) the prior distribution, (2) the posterior distribution after one update, and (3) the posterior distribution after two updates.  These are called the 90% **credible intervals** in this Bayesian setting, in contrast to the frequentist confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a few sentences commenting on similarities/differences between your original prior distribution (for the class data set), and the two updated posterior distributions, as well as the differences in the estimated credible intervals. Explain why the distributions/credible intervals are changing as they are, as more data are assimilated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Concluding note:** This only is technically accurate estimation if we assume that all of your socks are in the laundry at the same time. Or if we revise our prior distributions to estimate the number of socks we have in the laundry in any given load. But that's way harder to pin down.\n",
    "\n",
    "**Also:** Yes, I did wait to post this until after the Quizlet closed so I could include as much sock data as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br><br><br>\n",
    "\n",
    "<a id='helpers'></a>\n",
    "\n",
    "---\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "## Some things that might be useful\n",
    "\n",
    "Easiest way to start:  Click this cell, go to \"Cell\" in the toolbar above, and click \"Run All Below\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy as cp\n",
    "from scipy import stats\n",
    "from math import floor\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitely useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def slr(alpha, Teq, temperature):\n",
    "    '''sea-level emulator of Rahmstorf 2007 (DOI: 10.1126/science.1135456)\n",
    "    Takes global mean temperature as forcing, and parameters:\n",
    "    alpha = temperature sensitivity of sea level rise, and\n",
    "    Teq   = equilibrium temperature,\n",
    "    and calculates a rise/fall in sea levels, based on whether the temperature\n",
    "    is warmer/cooler than the equilibrium temperature Teq.\n",
    "    Here, we are only worrying about alpha (for now!)'''\n",
    "\n",
    "    n_time = len(temperature)\n",
    "    deltat = 1\n",
    "    sealevel = [0]*n_time\n",
    "    sealevel[0] = -134\n",
    "    for t in range(n_time-1):\n",
    "        sealevel[t+1] = sealevel[t] + deltat*alpha*(temperature[t]-Teq)\n",
    "\n",
    "    return sealevel\n",
    "\n",
    "\n",
    "def sock_model(n_socks, prop_pairs, n_picked):\n",
    "    \n",
    "    n_pairs = int( floor(n_socks/2) * prop_pairs)\n",
    "    n_odd = n_socks - 2*n_pairs\n",
    "\n",
    "    # label all our socks\n",
    "    socks = []\n",
    "    for i in range(n_pairs):\n",
    "        socks = socks + [i]*2\n",
    "    for j in range(n_pairs, n_pairs+n_odd):\n",
    "        socks = socks + [j]\n",
    "\n",
    "    if len(socks)>0:\n",
    "        # pick our socks out of the dryer\n",
    "        picked_socks = list(np.random.choice(socks, size=min(n_picked, n_socks), replace=False))\n",
    "        sock_counts = [picked_socks.count(k) for k in range(max(socks)+1)]\n",
    "\n",
    "        # how many were singleton socks?\n",
    "        unique = sock_counts.count(1)\n",
    "\n",
    "        # how many were in pairs?\n",
    "        pairs = sock_counts.count(2)\n",
    "        \n",
    "    else:\n",
    "        unique = 0\n",
    "        pairs = 0\n",
    "    \n",
    "    return unique, pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
