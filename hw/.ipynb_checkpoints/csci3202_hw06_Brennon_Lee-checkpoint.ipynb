{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top'></a>\n",
    "\n",
    "# CSCI 3202, Spring 2018:  Assignment 6\n",
    "### Due:  Friday 27 April 2018 by 12:00 PM\n",
    "\n",
    "### Your name:\n",
    "\n",
    "---\n",
    "\n",
    "**Note:** Some packages to load, helper functions and unit tests are defined at [the bottom of this notebook](#helpers). They're also defined up here, because I care.\n",
    "\n",
    "Shortcuts:  [top](#top) || [1](#p1) | [1a](#p1a) | [1b](#p1b) | [1c](#p1c) | [1d](#p1d) | [1e](#p1e) | [1f](#p1f) | [1g](#p1g) | [1h](#p1h) | [1i](#p1i) || [2](#p2) | [2a](#p2a) | [2b](#p2b) | [2c](#p2c) | [2d](#p2d) | [2e](#p2e) | [2f](#p2f) | [2g](#p2g) || [3](#p3) | [3a](#p3a) | [3b](#p3b) | [3c](#p3c) | [3d](#p3d) | [3e](#p3e) || [helpers](#helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import unittest\n",
    "\n",
    "def whichbin(x, left_edges, dx):\n",
    "    '''Function to return the bin number (starting from 0)\n",
    "    that the water balloon is. arguments include `x`, the\n",
    "    measured position of the balloon, and `left_edges`, an\n",
    "    array of the x-coordinates of the left edges of the bins.\n",
    "    Returns False if balloon is outside the domain.\n",
    "    '''\n",
    "    this_one = [k for k in range(len(left_edges)) if x >= left_edges[k] and x <= left_edges[k]+dx]\n",
    "    return this_one[0] if this_one else False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='p1'></a>[Back to top](#top)\n",
    "\n",
    "<img src=\"http://www.cs.colorado.edu/~tonyewong/home/resources/hw06_waterballoon.png\" style=\"width: 550px;\"/>\n",
    "\n",
    "## Problem 1:  Water balloon tracking\n",
    "\n",
    "Suppose you are walking down the street with your friend, when your sensors pick up a dangerous ***water balloon*** incoming! To track the water balloon and update your assessment of the probabilities that you or your friend will be hit, you think fast and implement a Hidden Markov Model. Unfortunately, you and your friend are also so busy implementing this model that you don't simply move out of the way (i.e., assume you are both stationary throughout this problem).\n",
    "\n",
    "First, you discretize the environment by using bins in the x-direction of width $\\Delta x = 0.1$ meters and time steps of 1/20 of a second. You do not need to consider any motion in the z-direction. The balloon starts 20 meters away in the y-direction at $(x_0, y_0)=(0.25,20)$, and is moving toward you at constant speed of $v = 2$ meters/second in the -y direction.\n",
    "\n",
    "Since we are dealing with a discretized world, it might be easier to work with the actual **bin numbers** as opposed to decimal values for x.  And we can use the helper function `whichbin` to determine which bin numbers correspond to which raw x coordinate values. So let's keep track of $(\\text{bin number}, y)$ tuple pairs as coordinates, where it should be clear from context whether we are referring to actual $x$ position or bin position.\n",
    "\n",
    "Now, in addition to this constant movement towards you, the water balloon also experiences some random fluctuations in its $x$ coordinate along its path. If the balloon is in bin $i$ in time step $t$, then in time step $t+1$ the balloon is:\n",
    "* still on course (in bin $i$) with probability $0.7$,\n",
    "* in either of the adjacent bins ($i\\pm 1$) with probability $0.1$, and\n",
    "* in either of the bins 2 away ($i\\pm 2$) with probability $0.05$.\n",
    "\n",
    "Your torso occupies the region $0 \\le x \\le 0.3$, and your friend occupies the region $0.4 \\le x \\le 0.7$. So there are exactly 3 bins that correspond to hitting you, and 3 that correspond to hitting your friend. Assume that when the balloon reaches $y=0$, if it is within either of these $x$ regions, then it will hit you or your friend. \n",
    "\n",
    "Your sensors (i.e., perception) are fairly inaccurate. When you measure the balloon's x-position, if the water balloon is actually in bin $i$, then:\n",
    "* you get it exactly correct 5/17 of the time,\n",
    "* you measure bins $i\\pm 1$ each with probability 3/17,\n",
    "* you measure bins $i\\pm 2$ each with probability 2/17, and\n",
    "* you measure bins $i\\pm 3$ each with probability 1/17.\n",
    "\n",
    "<br>\n",
    "\n",
    "<a/ id='p1a'></a>\n",
    "\n",
    "### (1a) \n",
    "\n",
    "First let's set up the discretized environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = 2                        # speed of the water balloon\n",
    "dt = 1/20                    # time step (seconds)\n",
    "dx = 0.1                     # bin width along x\n",
    "xb = np.arange(-1, 1.5, dx)  # x-coordinates of left-edges of bins\n",
    "n_bins = len(xb)             # total number of bins\n",
    "initial_location = (whichbin(0.25, xb, dx), 20)  # initial (bin, y) location of balloon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which bin numbers would the balloon need to be in in order to hit you at $y=0$?  What about your friend?  Store this information in some kind of variable - you may need to use it later.\n",
    "\n",
    "Given the time step, speed in the y-direction, and initial distance of the water balloon, how many time steps, $T$, are there until the balloon reaches $y=0$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a/ id='p1b'></a>\n",
    "\n",
    "### (1b)\n",
    "\n",
    "Set up a `Balloon` class.  Include attributes for:\n",
    "* balloon speed\n",
    "* the probabilities associated with changes in the balloon's x-position (by bin) from one time step to the next (i.e., the Markov transition probabilities)\n",
    "* the current balloon location\n",
    "* the time step for the simulation\n",
    "\n",
    "and methods for:\n",
    "* updating the x- and y-positions of the balloon between time steps. The x-position that you track should be the bin.\n",
    "* declaring (through Boolean output) whether or not the balloon has arrived at $y=0$. This one might require you to carefully account for any floating point operations that lead to a non-integer y-position being compared against the integer 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Balloon:\n",
    "    \n",
    "    # your code goes here... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a/ id='p1c'></a>\n",
    "\n",
    "### (1c)\n",
    "\n",
    "Set up a `Sensor` class to represent your ability to measure the balloon's position. Include an attribute for the sensor model probabilities, as well as a method `measure`, to return the measured $(x,y)$ pair as a tuple. The x-coordinate, however, should be the bin that you perceived the balloon to be in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sensor:\n",
    "\n",
    "    # your code goes here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a/ id='p1d'></a>\n",
    "\n",
    "### (1d)\n",
    "\n",
    "Generate a simulated Markov chain sequence of water balloon states (bins along x-direction, $X$) and your corresponding measurements of those states ($E$). Make a plot of these two time series on the same axes, and be sure to include axis labels and a legend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a/ id='p1e'></a>\n",
    "\n",
    "### (1e)\n",
    "\n",
    "Recall that `Forward` filtering updates our estimate of the probability distribution for $X_t$ in light of *all* of the evidence up to time $t$, $E_{1:t}$, as\n",
    "\n",
    "$$f_{1:t} = \\alpha ~ \\texttt{forward}(f_{1:t-1}, E_t)$$\n",
    " \n",
    "and the `Backward` function is a necessary step to update our estimate of the probability distribution for $X_k$ (some $k$ in $[0,t]$) in light of *all* of the evidence that occurred after $k$, $E_{k+1:t}$, as:\n",
    "\n",
    "$$b_{k+1:t} = \\texttt{backward}(b_{k+2:t}, E_{k+1})$$\n",
    "\n",
    "where the specific functional forms of these recurrences is given in the textbook and lecture slides.\n",
    "\n",
    "Code up `forward` and `backward` functions, taking in as arguments:\n",
    "* `f1` or `b1`, the relevant previous (forward) or future (backward) \"probability distribution\" from the last recursive function call, respectively,\n",
    "* `evidence`, which is the single observation that you are assimilating,\n",
    "* `sens`, a `Sensor` object,\n",
    "* `ball`, a `Balloon` object, and\n",
    "* `xb`, some way to denote where the bins or x-coordinates are (which is more convenient may depend on how you stored your sensor and Markov transition probabilities).\n",
    "\n",
    "Note that `forward` should return *all* of the probability distributions, at each time step, because you will need these for `backward`, which should return a single probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(f1, evidence, sens, ball, xb):\n",
    "    \n",
    "    # your code goes here...\n",
    "    \n",
    "    return f\n",
    "\n",
    "\n",
    "def backward(b1, evidence, sens, ball, xb):\n",
    "    \n",
    "    # your code goes here...\n",
    "    \n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a/ id='p1f'></a>\n",
    "\n",
    "### (1f)\n",
    "\n",
    "**For the remainder of this problem**, use the data set [data_waterballoon.csv](https://piazza.com/class_profile/get_resource/jc4v74a5uu5wa/jfxh1l8kelo2pv), available on Piazza under the Resources tab and linked here. The first column ($X$) corresponds to the actual bin number in which the water balloon is located during the time step corresponding to that row in the data table, and the second column ($E$) is the associated measurement. Note that there is no measurement associated with the initial location. You should use a uniform prior distribution for the initial location, over the actual location and one bin to either side.\n",
    "\n",
    "Use your `forward` and `backward` functions to implement the forward-backward algorithm to obtain both filtered estimates and smoothed estimates of the probability distribution for water balloon location for all times after the initial time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the filtered estimate at $t=1$ (first time step, not 1 second) differ from what you would estimate based only on the Markov model, without knowledge of $E_1$, your first measurement of balloon position?  To solve for the Markov model probability distribution $P(X_1)$, you can either use enumeration or calculate the probabilities by hand. (By hand isn't too bad, because there is nice symmetry you can take advantage of, so you only need to actually calculate a few things.)\n",
    "\n",
    "Plot the two probability distributions on the same set of axes and comment on the differences. Be sure to include appropriate axis labels and a legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a/ id='p1g'></a>\n",
    "\n",
    "### (1g)\n",
    "\n",
    "Based on the smoothed estimates for the probability distribution of the water balloon at the moment the balloon reaches $y=0$, calculate the following:\n",
    "* the probability that the balloon hits you,\n",
    "* the probability that the balloon hits your friend, and\n",
    "* the probability that the balloon misses both of you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a/ id='p1h'></a>\n",
    "\n",
    "### (1h)\n",
    "\n",
    "Make a plot of the smoothed estimates of the probability that the balloon will hit you, as a function of time step, and of the probability that the balloon will hit your friend. Include axis labels and a legend.  Write a few sentences commenting on the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a/ id='p1i'></a>\n",
    "\n",
    "### (1i)\n",
    "\n",
    "Give an example of an improvement we could make to the sensor model so that it would better represent the perception of a typical human being. Your answer should include both a revision of the code snippet below, as well as a few sentences justifying your code modifications. Note that you do **not** need to run this code; ***Pythonic*** pseudocode is fine.\n",
    "\n",
    "```\n",
    "class Sensor:    \n",
    "    def measure(self, balloon):\n",
    "    \n",
    "        # your code goes here\n",
    "    \n",
    "        return (x_meas, y_meas)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<a/ id='p2'></a>\n",
    "\n",
    "---\n",
    "\n",
    "<img src='https://www.explainxkcd.com/wiki/images/5/5f/interaction.png' style=\"width: 600px;\"/>\n",
    "## Problem 2: Navigating an awkward situation with grace and poise\n",
    "\n",
    "Suppose you are at a social event where you would like to avoid any interaction with a large number of the other attendees. It's not that you don't like them, it's just that you don't like *talking to* them. A few of your good friends are also in attendance, but they are tucked away in a corner. The rectangular room in which the event is being held spans gridcells at $x=1,2,\\ldots, 6$ and $y=1,2,\\ldots, 5$. At the eastern edge ($x=6$) of this first floor room, there is a balcony, with a 6-foot drop. If the event becomes unbearably awkward, you can jump off the balcony and run away. Of course, this might hurt a little bit, so we should incorporate this into our reward structure.\n",
    "\n",
    "The terminal states and rewards associated with them are given in the diagram below. The states are represented as $(x,y)$ tuples. The available actions in non-terminal states include moving exactly 1 unit North (+y), South (-y), East (+x) or West (-x), although you should not include walking into walls, because that would be embarrassing in front of all these other people. Represent actions as one of 'N', 'S', 'E', or 'W'. For now, assume all non-terminal states have a default reward of -0.01, and use a discount factor of 0.99.\n",
    "\n",
    "<img src=\"http://www.cs.colorado.edu/~tonyewong/home/resources/hw06_mdp.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "Use the following transition model for this decision process, if you are trying to move from state $s$ to state $s'$:\n",
    "* you successfully move from $s$ to $s'$ with probability 0.6\n",
    "* the remaining 0.4 probability is spread equally likely across state $s$ **and** all adjacent (N/S/E/W) states except for $s'$. Note that this does not necessarily mean that all adjacent states have 0.1, because some states do not have 4 adjacent states.\n",
    "\n",
    "<a/ id='p2a'></a>\n",
    "\n",
    "### (2a)\n",
    "\n",
    "Complete the `MDP` class below. The docstring comments provide some desired specifications. You may add additional methods or attributes, if you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self, nrow, ncol, terminal, default_reward, discount):\n",
    "        '''Create/store the following attributes:\n",
    "        states -- list of all the states (x,y) tuples\n",
    "        terminal_states -- is a dictionary with terminal state keys, and rewards as values\n",
    "        default_reward -- is the reward for being in any non-terminal state\n",
    "        df -- discount factor\n",
    "        ... and anything else you decide will be useful!\n",
    "        '''\n",
    "        \n",
    "        # your code goes here...\n",
    "        \n",
    "\n",
    "    def actions(self, state):\n",
    "        '''Return a list of available actions from the given state.\n",
    "        [None] are the actions available from a terminal state.\n",
    "        '''\n",
    "        \n",
    "        # your code goes here...\n",
    "        \n",
    "        \n",
    "    def reward(self, state):\n",
    "        '''Return the reward for being in the given state'''\n",
    "        \n",
    "        # your code goes here...\n",
    "        \n",
    "        \n",
    "    def result(self, state, action):\n",
    "        '''Return the resulting state (as a tuple) from doing the given\n",
    "        action in the given state, without uncertainty. Uncertainty\n",
    "        is incorporated into the transition method.\n",
    "        state -- a tuple representing the current state\n",
    "        action -- one of N, S, E or W, as a string\n",
    "        '''\n",
    "    \n",
    "        # your code goes here...\n",
    "        \n",
    "                \n",
    "    def transition(self, state, action):\n",
    "        '''Return the probabilities and subsequent states associated\n",
    "        with taking the given action from the given state. Can be done\n",
    "        however you want, so that it works with your value/policy iteration.\n",
    "        '''\n",
    "        \n",
    "        # your code goes here...\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now:** create an `MDP` object to represent the decision process in this problem.\n",
    "\n",
    "To test and get comfortable with your `MDP` class methods and attributes, and making the relevant calculations with this structure, calculate the expected utility of walking north from (1,1). Assume initially that all states $(x,y)$ have a utility of $x+y$ (including the terminal states)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit tests\n",
    "Note that these are non-exhaustive, because there is some flexibility in how the `transition` method works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_to_run = unittest.TestLoader().loadTestsFromModule(Tests_Problem2())\n",
    "unittest.TextTestRunner().run(tests_to_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a/ id='p2b'></a>\n",
    "\n",
    "### (2b)\n",
    "\n",
    "Implement value iteration to calculate the utilities for each state.  Also implement a function that takes as arguments an `MDP` object and a dictionary of state-utility pairs (key-value) and returns a dictionary for the optimal policy.  The optimal policy dictionary should have state tuples as keys and the optimal move (None, N, S, E or W) as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_iteration(mdp, tol=1e-3):\n",
    "    \n",
    "    # your code goes here...\n",
    "    \n",
    "\n",
    "def find_policy(mdp, utility):\n",
    "    \n",
    "    # your code goes here...\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now actually use your `value_iteration` and `find_policy` functions to calculate the utility for each state in this MDP, and the optimal action in each state.\n",
    "\n",
    "As a sanity check, print the utilities of these terminal states:\n",
    "1. `utility[(1,5)]`\n",
    "1. `utility[(6,1)]`\n",
    "\n",
    "and print the utility of these states that are nearby to terminal states, so their utilities should be similar to the nearby terminal states' utilities:\n",
    "1. `utility[(2,5)]`\n",
    "1. `utility[(5,3)]`\n",
    "\n",
    "And print the policy for these states to make sure they make sense:\n",
    "1. `policy[(2,4)]`\n",
    "1. `policy[(1,1)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a/ id='p2c'></a>\n",
    "\n",
    "### (2c)\n",
    "\n",
    "If we enter the room at (5,1), what is the optimal path for us to follow?  Create a graphic to illustrate this policy pathway, either by generating a plot in Python (like the maze solution path) or by uploading a hand-drawn image and including it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a/ id='p2d'></a>\n",
    "\n",
    "### (2d)\n",
    "\n",
    "From (3,2) the optimal move is to walk West. If we are trying to go talk to our friends in the Northwest corner, why would we rather do this than walk North first, then West?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a/ id='p2e'></a>\n",
    "\n",
    "### (2e)\n",
    "\n",
    "How painfully awkward do you need to set the default reward for non-terminal states before the optimal move from (5,1) becomes jumping off the balcony immediately and running away?  Round your answer to two decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a/ id='p2f'></a>\n",
    "\n",
    "### (2f)\n",
    "\n",
    "In **2e** we assumed a certain level of loss (negative reward) just for being present.  But a more realistic approach might be to instead change the reward structure for the terminal states. Consider the terminal states with -1 reward in the default model. Let $R^*$ denote the reward associated with these states. How low does $R^*$ need to be in order for us to immediately jump off the balcony and run away? Use the default non-terminal state reward of -0.01. Write a few sentences interpreting your result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a/ id='p2g'></a>\n",
    "\n",
    "### (2g)\n",
    "\n",
    "Given the problem context, write a few sentences about why this is or is not an appropriate transition model. Include an interpretation of the terminal states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<a/ id='p3'></a>\n",
    "\n",
    "## Problem 3: your very own MDP\n",
    "\n",
    "For this problem, you do not need to write any code, but rather communicate your ideas clearly using complete sentences and descriptions of the concepts the questions ask about. You can, of course, include some pseudocode if it helps, but that is not strictly necessary.\n",
    "\n",
    "<a/ id='p3a'></a>\n",
    "\n",
    "### (3a)\n",
    "\n",
    "Describe something you think would be interesting to model using a Markov decision process.  Be **creative** - do not use any examples from your homework, class, or the textbook, and if you are working with other students, please **come up with your own example**. There are so, SO many possible answers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a/ id='p3b'></a>\n",
    "\n",
    "### (3b)\n",
    "\n",
    "What are the states associated with your MDP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a/ id='p3c'></a>\n",
    "\n",
    "### (3c)\n",
    "\n",
    "What is the reward structure associated with your MDP?  Include a discussion of terminal/non-terminal states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a/ id='p3d'></a>\n",
    "\n",
    "### (3d)\n",
    "\n",
    "What are the actions and transition model associated with your MDP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a/ id='p3e'></a>\n",
    "\n",
    "### (3e)\n",
    "\n",
    "Interpret what an optimal policy represents in the context of your particular MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "\n",
    "<a id='helpers'></a>\n",
    "\n",
    "---\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "## Some things that might be useful\n",
    "\n",
    "Easiest way to start:  Click this cell, go to \"Cell\" in the toolbar above, and click \"Run All Below\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import unittest\n",
    "\n",
    "def whichbin(x, left_edges, dx):\n",
    "    '''Function to return the bin number (starting from 0)\n",
    "    that the water balloon is. arguments include `x`, the\n",
    "    measured position of the balloon, and `left_edges`, an\n",
    "    array of the x-coordinates of the left edges of the bins.\n",
    "    Returns False if balloon is outside the domain.\n",
    "    '''\n",
    "    this_one = [k for k in range(len(left_edges)) if x >= left_edges[k] and x <= left_edges[k]+dx]\n",
    "    return this_one[0] if this_one else False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tests_Problem2(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        nrow = 3\n",
    "        ncol = 3\n",
    "        default_reward = -0.2\n",
    "        discount = 0.5\n",
    "        terminal = {(1,3):-1, (1,2):2}\n",
    "        self.mdp = MDP(nrow, ncol, terminal, default_reward, discount)\n",
    "    def test_actions_some(self):\n",
    "        self.assertEqual(set(self.mdp.actions((2,2))) == {'N','S','E','W'}, True)\n",
    "    def test_actions_few(self):\n",
    "        self.assertEqual(set(self.mdp.actions((1,1))) == {'N','E'}, True)\n",
    "    def test_actions_none(self):\n",
    "        self.assertEqual(set(self.mdp.actions((1,2))) == {None}, True)\n",
    "    def test_reward_t(self):\n",
    "        self.assertEqual(self.mdp.reward((1,2)) == 2, True)\n",
    "    def test_reward_nt(self):\n",
    "        self.assertEqual(self.mdp.reward((2,2)) == -0.2, True)\n",
    "    def test_result_nt(self):\n",
    "        self.assertEqual(self.mdp.result((1,1), 'N') == (1,2), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
