{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 3202, Spring 2018: Practicum\n",
    "\n",
    "---\n",
    "\n",
    "<a id='top'></a>\n",
    "\n",
    "This practicum is due on Moodle by **4 PM on Friday 4 May 2018**.  Your solutions to theoretical questions should be done in Markdown/LateX directly below the associated question. Your solutions to computational questions should include any relevant Python code, as well as results and any written commentary.\n",
    "\n",
    "**The rules:**\n",
    "\n",
    "1. All work, code and analysis must be **your own**.\n",
    "1. You may use your course notes, posted lecture slides, in-class notebooks and homework solutions as resources.  You may also search online for answers to general knowledge questions, like the form of a probability distribution function, or how to perform a particular operation in Python.\n",
    "1. You may **not** post to message boards or other online resources asking for help.\n",
    "1. **You may not collaborate with classmates or anyone else.**\n",
    "1. This is meant to be like a coding portion of your final exam. So, I will be much less helpful than I typically am with homework. For example, I will not check answers, help debug your code, and so on.\n",
    "1. If you have a question, post it first as a **private** Piazza message. If I decide that it is appropriate for the entire class, then I will make it a public post (and anonymous).\n",
    "1. If something is left open-ended, it is probably because I intend for you to code it up however you want, and only care about the plots/analysis I see at the end. Feel free to ask clarifying questions though.\n",
    "\n",
    "Violation of these rules will result in an **F** and a trip to the Honor Code council.\n",
    "\n",
    "---\n",
    "**By writing your name below, you agree to abide by these rules:**\n",
    "\n",
    "**Your name: Brennon Lee**\n",
    "\n",
    "---\n",
    "\n",
    "Shortcuts:  [Top](#top) || [1. Search](#p1) | [2. Bayes nets](#p2) | [3. Markov chains](#p3) | [4. Reinforcement learning](#p4) || [Bottom](#bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a/ id='p1'></a>\n",
    "\n",
    "---\n",
    "## [25 pts] Problem 1:  Route-finding\n",
    "\n",
    "Consider the map of the area to the west of the Engineering Center given below, with a fairly coarse Cartesian grid superimposed.\n",
    "\n",
    "<img src=\"http://www.cs.colorado.edu/~tonyewong/home/resources/engineering_center_grid_zoom.png\" style=\"width: 800px;\"/>\n",
    "\n",
    "The green square at $(x,y)=(1,15)$ is the starting location, and you would like to walk from there to the yellow square at $(25,9)$ with the **lowest total path cost**. The filled-in blue squares are obstacles, and you cannot walk through those locations.  You also cannot walk outside of this grid.\n",
    "\n",
    "Legal moves in the North/South/East/West directions have a step cost of 1. Moves in the diagonal direction (for example, from $(1,15)$ to $(2,14)$) are allowed, but they have a step cost of $\\sqrt{2}$. \n",
    "\n",
    "Of course, you can probably do this problem without a search algorithm. But (1) that will provide a useful \"sanity check\" for your answer, and (2) an AI agent cannot necessarily figure that out... without your help!\n",
    "\n",
    "#### Part A\n",
    "Write a function `adjacent_states(state)`:\n",
    "* takes a single argument `state`, which is a tuple representing a valid state in this state space\n",
    "* returns in some form the states reachable from `state` and the step costs. How exactly you do this is up to you.\n",
    "\n",
    "Print to the screen the output for `adjacent_states((1,15))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_black_list():\n",
    "    black_list = []\n",
    "    for row in range(2,15):\n",
    "        black_list.append((1,row))\n",
    "    for row in range(3,14):\n",
    "        black_list.append((2,row))\n",
    "    for row in range(4, 13):\n",
    "        black_list.append((3,row))\n",
    "    for row in range(5, 12):\n",
    "        black_list.append((4,row))\n",
    "\n",
    "    for row in range(9,13):\n",
    "        black_list.append((10,row))\n",
    "    for row in range(9,15):\n",
    "        black_list.append((11,row))\n",
    "    for row in range(9,15):\n",
    "        black_list.append((12,row))\n",
    "    for row in range(9,15):\n",
    "        black_list.append((13,row))\n",
    "    for row in range(11,15):\n",
    "        black_list.append((14,row))\n",
    "        \n",
    "    for col in range(5, 24):\n",
    "        black_list.append((col, 1))\n",
    "        \n",
    "    for row in range(11,17):\n",
    "        for col in range(21,26):\n",
    "            black_list.append((col, row))\n",
    "    return black_list\n",
    "\n",
    "def generate_board():\n",
    "    board = []\n",
    "    black_list = generate_black_list()\n",
    "    for i in range(1, 26):\n",
    "        for j in range(1, 17):\n",
    "            if (i,j) not in black_list:\n",
    "                board.append((i,j))\n",
    "    return board\n",
    "\n",
    "def adjacent_states(state):\n",
    "    possible_states = []\n",
    "    board = generate_board()\n",
    "\n",
    "    # check NWSE moves\n",
    "    if ((state[0] + 1, state[1]) in board): # can move right?\n",
    "        move = (state[0]+1, state[1])\n",
    "        possible_states.append((move, 1))\n",
    "    if ((state[0] - 1, state[1]) in board): # can move left?\n",
    "        move = (state[0]-1, state[1])\n",
    "        possible_states.append((move, 1))\n",
    "    if ((state[0], state[1] + 1) in board): # can move up?\n",
    "        move = (state[0], state[1]+1)\n",
    "        possible_states.append((move, 1))\n",
    "    if ((state[0], state[1] - 1) in board): # can move down?\n",
    "        move = (state[0], state[1]-1)\n",
    "        possible_states.append((move, 1))\n",
    "    \n",
    "    # check diagonal moves\n",
    "    if ((state[0]+1, state[1]+1) in board): # can move NE?\n",
    "        move = (state[0]+1, state[1]+1)\n",
    "        possible_states.append((move, np.sqrt(2)))\n",
    "    if ((state[0]-1, state[1]+1) in board): # can move NW?\n",
    "        move = (state[0]-1, state[1]+1)\n",
    "        possible_states.append((move, np.sqrt(2)))\n",
    "    if ((state[0]+1, state[1]-1) in board): # can move SE?\n",
    "        move = (state[0]+1, state[1]-1)\n",
    "        possible_states.append((move, np.sqrt(2)))\n",
    "    if ((state[0]-1, state[1]-1) in board): # can move SW?\n",
    "        move = (state[0]-1, state[1]-1)\n",
    "        possible_states.append((move, np.sqrt(2)))\n",
    "        \n",
    "    return possible_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((2, 15), 1),\n",
       " ((1, 16), 1),\n",
       " ((2, 16), 1.4142135623730951),\n",
       " ((2, 14), 1.4142135623730951)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjacent_states((1,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B\n",
    "Three candidate heuristic functions might be:\n",
    "1. `heuristic_cols(state, goal)` = number of columns between the argument `state` and the `goal`\n",
    "1. `heuristic_rows(state, goal)` = number of rows between the argument `state` and the `goal`\n",
    "1. `heuristic_eucl(state, goal)` = Euclidean distance between the argument `state` and the `goal`\n",
    "\n",
    "Write a function `heuristic_max(state, goal)` that returns the maximum of all three of these heuristic functions for a given `state` and `goal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristic_cols(state, goal):\n",
    "    return goal[0] - state[0]\n",
    "\n",
    "def heuristic_rows(state, goal):\n",
    "    return goal[1] - state[1]\n",
    "\n",
    "def heuristic_eucl(state, goal):\n",
    "    return np.sqrt((goal[0] - state[0])**2 + (goal[1] - state[1])**2)\n",
    "\n",
    "def heuristic_max(state,goal):\n",
    "    return max(heuristic_cols(state,goal), heuristic_rows(state,goal), heuristic_eucl(state,goal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part C\n",
    "Is the Manhattan distance an admissible heuristic function for this problem?  Explain why or why not. A counterexample is a great way to prove that something is not true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Come back to this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part D\n",
    "Use A\\* search and the `heuristic_max` heuristic to find the shortest path from the initial state at $(1,15)$ to the goal state at $(25,9)$. Your search **should not** build up the entire state space graph in memory. Instead, use the `adjacent_states` function from Part A, similarly to the 8-tile problem from Homework 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Frontier_PQ_astar:\n",
    "    def __init__(self, start, cost):\n",
    "        self.start = start\n",
    "        self.cost = cost\n",
    "        self.states= {} # dic of states on fronter w/ min path cost to arrive\n",
    "        self.q = [] # tuple list of (cost, state)\n",
    "        \n",
    "    def add(self, state, cost, hcost):\n",
    "        self.states[state] = cost\n",
    "        heapq.heappush(self.q, (hcost + cost, state))\n",
    "        return \n",
    "    \n",
    "    def pop(self):\n",
    "        return heapq.heappop(self.q)\n",
    "    \n",
    "    def replace(self, state, cost, hcost):\n",
    "        del self.states[state]\n",
    "        self.states[state] = cost\n",
    "        heapq.heapreplace(self.q, (hcost + cost, state))\n",
    "        return \n",
    "\n",
    "def astar_search(start, goal, heuristic, return_cost, return_nexp):\n",
    "    explored = {}\n",
    "    frontier = Frontier_PQ_astar(start, 0)\n",
    "    \n",
    "    explored[start] = (None)\n",
    "    frontier.add(start, 0, heuristic(start, goal))\n",
    "    nexp = 0\n",
    "\n",
    "    while frontier.q:\n",
    "        parentTcost, parent = frontier.pop()\n",
    "        nexp += 1\n",
    "        if (parent == goal):\n",
    "            goalPath = path(explored, goal)\n",
    "            if not return_nexp:\n",
    "                return (goalPath, pathcost(goalPath, state_graph)) if return_cost else goalPath\n",
    "            else:\n",
    "                return (goalPath, pathcost(goalPath, state_graph), nexp) if (return_cost, nexp) else goalPath\n",
    "        \n",
    "        moves  = adjacent_states(parent)\n",
    "        for move in moves:\n",
    "            move_tuple = move[0]\n",
    "            if move_tuple not in explored: # we havent made this move before\n",
    "                tempPath = {}\n",
    "                tempPath[parent] = (None)\n",
    "                tempPath[move_tuple] = parent\n",
    "                g_n = frontier.states[parent] + 1 # plus one for parent to child\n",
    "                h_n = heuristic(move_tuple, goal)\n",
    "                f_n = g_n + h_n\n",
    "\n",
    "                if move_tuple in frontier.states:\n",
    "                    if (g_n < frontier.states[move_tuple]):\n",
    "                        frontier.replace(move_tuple, g_n, h_n)\n",
    "                        explored[move_tuple] = parent\n",
    "                if move_tuple not in frontier.states:\n",
    "                    explored[move_tuple] = parent\n",
    "                    childCost = len(explored) - 1\n",
    "                    frontier.add(move_tuple, childCost, h_n)\n",
    "                \n",
    "    return\n",
    "\n",
    "def path(previous, s): \n",
    "    '''\n",
    "    `previous` is a dictionary chaining together the predecessor state that led to each state\n",
    "    `s` will be None for the initial state\n",
    "    otherwise, start from the last state `s` and recursively trace `previous` back to the initial state,\n",
    "    constructing a list of states visited as we go\n",
    "    '''\n",
    "    if s is None:\n",
    "        return []\n",
    "    else:\n",
    "        return path(previous, previous[s])+[s]\n",
    "\n",
    "def pathcost(path, step_costs):\n",
    "    '''\n",
    "    add up the step costs along a path, which is assumed to be a list output from the `path` function above\n",
    "    '''\n",
    "    cost = 0\n",
    "    for s in range(len(path)-1):\n",
    "        cost += step_costs[path[s]][path[s+1]]\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 15),\n",
       " (2, 15),\n",
       " (3, 15),\n",
       " (4, 14),\n",
       " (5, 13),\n",
       " (6, 12),\n",
       " (7, 11),\n",
       " (8, 10),\n",
       " (9, 9),\n",
       " (10, 8),\n",
       " (11, 8),\n",
       " (12, 8),\n",
       " (13, 8),\n",
       " (14, 8),\n",
       " (15, 8),\n",
       " (16, 8),\n",
       " (17, 8),\n",
       " (18, 8),\n",
       " (19, 8),\n",
       " (20, 8),\n",
       " (21, 8),\n",
       " (22, 8),\n",
       " (23, 8),\n",
       " (24, 8),\n",
       " (25, 9)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "astar_search((1,15), (25,9), heuristic_max, False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part E\n",
    "Make a figure depicting the optimal route from the initial state to the goal, similarly to how you depicted the maze solution in Homework 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-b2ab1f508013>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mplot_maze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mastar_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheuristic_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-44-b2ab1f508013>\u001b[0m in \u001b[0;36mplot_maze\u001b[0;34m(maze, Path)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mmaze\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaze\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "from matplotlib import colors\n",
    "\n",
    "map_ = np.array(generate_board())\n",
    "\n",
    "\n",
    "def maze_to_graph(maze):\n",
    "    yValues = maze.shape[0]\n",
    "    xValues = maze.shape[1]\n",
    "    dictionary = {}\n",
    "    for y in range(0, yValues):\n",
    "        for x in range(0,xValues):\n",
    "            if(maze[y][x] == 0.0):\n",
    "                \n",
    "                dictionary[(x,y)] = {}\n",
    "                \n",
    "                if (maze[y+1][x] == 0.0): #north\n",
    "                    dictionary[(x,y)][(x,y+1)] = 'N'\n",
    "                    \n",
    "                if (maze[y-1][x] == 0.0): #south\n",
    "                    dictionary[(x,y)][(x,y-1)] = 'S'\n",
    "                    \n",
    "                if (maze[y][x+1] == 0.0): #west\n",
    "                    dictionary[(x,y)][(x+1,y)] = 'E'\n",
    "                    \n",
    "                if (maze[y][x-1] == 0.0): #east\n",
    "                    dictionary[(x,y)][(x-1,y)] = 'W'\n",
    "    return(dictionary)\n",
    "\n",
    "def plot_maze(maze, Path=None):\n",
    "    graph = maze_to_graph(maze)\n",
    "    cmap = colors.ListedColormap(['orange', 'grey', 'green'])\n",
    "    \n",
    "    for (i,j) in Path:\n",
    "        maze[j][i] = 2   \n",
    "    \n",
    "    plt.matshow(np.flip(maze, axis=0), cmap=cmap)\n",
    "    return\n",
    "\n",
    "plot_maze(map_, astar_search((1,15), (25,9), heuristic_max, False, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#top)\n",
    "\n",
    "<a/ id='p2'></a>\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "## [25 pts] Problem 2:  Bayesian networks\n",
    "\n",
    "#### Part A\n",
    "Suppose we are about to go on a run and are trying to decide how far to run. Consider the Bayesian network depicted below for this decision. The length of our run ($R$, miles) is continuous and depends on the temperature ($T$, degrees Fahrenheit), which is also a continuous variable, and whether or not it is raining ($P$), which is a discrete Boolean variable.\n",
    "\n",
    "<img src=\"http://www.cs.colorado.edu/~tonyewong/home/resources/running_bayesnet.png\" style=\"width: 450px;\"/>\n",
    "\n",
    "1. Read in the data set [`data_running.csv`](https://piazza.com/class_profile/get_resource/jc4v74a5uu5wa/jfv207i7dlqd8) from the Resources tab on Piazza (and linked here).\n",
    "1. How many data points are there?\n",
    "1. Make a histogram of the distribution of temperature, with appropriate axis labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B\n",
    "\n",
    "Decide on appropriate prior distributions for $P$ and $T$.  Note that $P$ is discrete (Boolean), while $T$ is continuous.  There is not necessarily only one correct response; your answer should be fully justified by calculations shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part C\n",
    "\n",
    "1. Make two scatter plots:  one of the relationship between temperature (x-axis) and run length (y-axis) for only the days when it was raining, and one of this relationship for only the days when it was not raining. Label your axes appropriately.\n",
    "1. Run length is necessarily non-negative, so a log-normal distribution would be appropriate as the conditional probability distribution for $R$, given $P$ and $T$. Let the log-normal parameter related to central tendency vary linearly with $T$, and categorically depending on whether or not it is raining. Keep the log-normal parameter related to distribution spread fixed. \n",
    "1. Report the values of all relevant estimated parameters for the conditional probability distribution. There is not necessarily only one correct response; your answer should be fully justified by calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part D\n",
    "\n",
    "Find the probability distributions for your run length on a day that is 80 degrees Fahrenheit and **not** raining, as well as run length on an 80-degree day when it **is** raining.  Plot these two distributions on the same set of axes, with run length (miles) on the x-axis and probability density on the y-axis. Include a legend and appropriate axis labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part E\n",
    "\n",
    "Use approximate Bayesian computation (i.e., sampling) to estimate the probability of going on a run **at least** 8 miles in length. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#top)\n",
    "\n",
    "<a/ id='p3'></a>\n",
    "\n",
    "---\n",
    "## [25 pts] Problem 3:  Markov chain Monte Carlo\n",
    "\n",
    "Consider the problem of estimating the slope and intercept parameters for a linear model. Suppose this linear model represents the amount of ice cream you will consume ($y$, in pints) after going on a bike ride of length $x$ (in miles).\n",
    "\n",
    "$$y = \\alpha x + \\beta$$\n",
    "\n",
    "#### Part A\n",
    "\n",
    "Fix the $x$ distance values at which we will estimate our ice cream intake covering the 0 to 9.9 range (inclusive), at intervals of 0.1 miles. Let $\\alpha^* = 0.5$ and $\\beta^* = 8$ represent the true parameters for your linear model. Generate a synthetic data set of ice cream eaten for each bike ride length $x$, assuming that measured ice cream consumption is normally distributed, centered at the true model and with standard deviation $\\sigma = 1$. Create a scatter plot of you true model and the \"observed\" data, with appropriate axis labels and a legend.\n",
    "\n",
    "**Question:** How many data points are there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B\n",
    "\n",
    "In class, we used the posterior score itself to calculate acceptance probabilities at iteration $t$ for the Metropolis-Hastings algorithm:\n",
    "\n",
    "$$p_{accept} = \\dfrac{\\pi(\\theta_{new} \\mid y_{meas})}{\\pi(\\theta_{t} \\mid y_{meas})},$$\n",
    "\n",
    "where $\\theta_t = (\\alpha_t, T_{eq,t})$ are the parameter estimates in our Markov chain at iteration $t$ and $y_{meas}$ is the data.  But it is much more numerically stable to use the natural logarithm of the posterior score in most applications. \n",
    "\n",
    "1. Define a log-posterior function to return the logarithm of the posterior score for a given set of parameter values, varying *both* $\\alpha$ and $\\beta$. Use uniform prior distributions, for $\\alpha \\in [0,5]$ and $\\beta \\in [0,15]$. You may assume that $\\alpha$ and $\\beta$ are independent, so their prior distribution factors as $\\pi(\\alpha, \\beta) = \\pi(\\alpha) \\pi(\\beta)$.\n",
    "2. Calculate the simple least squares regression estimates of $\\alpha$ and $\\beta$, and report their log-posterior score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part C\n",
    "\n",
    "To use the log-posterior as our objective function, we need to modify the Metropolis-Hastings acceptance probability so we can calculate it in terms of posterior scores (the right hand side below):\n",
    "\n",
    "$$\\log{(p_{accept})} = \\log{\\left(\\dfrac{\\pi(\\theta_{new} \\mid y_{meas})}{\\pi(\\theta_{t} \\mid y_{meas})}\\right)} = \\log{(\\pi(\\theta_{new} \\mid y_{meas}))} - \\log{(\\pi(\\theta_{t} \\mid y_{meas}))}$$\n",
    "\n",
    "If you exponentiate both sides, things should start to look familiar from simulated annealing:\n",
    "\n",
    "$$\\exp{\\left[\\log{(p_{accept})}\\right]} = p_{accept} = \\exp{\\left[\\log{(\\pi(\\theta_{new} \\mid y_{meas}))} - \\log{(\\pi(\\theta_{t} \\mid y_{meas}))}\\right]}$$\n",
    "\n",
    "Implement this in the Metropolis-Hastings algorithm.  Use a multivariate normal distribution to propose new parameters $(\\alpha, \\beta)$ jointly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part D\n",
    "\n",
    "Obtain samples from the posterior distribution of $\\alpha$ and $\\beta$ by running the Metropolis-Hastings algorithm for 5000 iterations.  Start with the least squares estimates from Part B as the initial values for each parameter. The step sizes for the parameters should be chosen such that the acceptance rate is somewhere between 23 and 44%. Remove the first half of each Markov chain for burn-in, and save the rest for analysis.\n",
    "\n",
    "1. Create a two-panel figure of the history plots of your Markov chains for each of $\\alpha$ and $\\beta$. Calculate and report the acceptance rate.\n",
    "1. Create a two-panel figure of the histograms for your posterior samples for analysis for $\\alpha$ and $\\beta$. Label your axes appropriately.\n",
    "1. Below your histogram, report on the 5-95% credible intervals (percentiles) for each parameter, as well as their posterior medians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part E\n",
    "\n",
    "Now repeat this experiment, but only include a subsample of 50 data points out of the original set. Generate the same set of plots and diagnostic numbers as Part D. You may need to adjust the step sizes in order to obtain similar acceptance rates.\n",
    "\n",
    "Compare the 5-95% credible intervals found in Part D and E, and comment on any differences you find."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#top)\n",
    "\n",
    "<a/ id='p4'></a>\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "## [25 pts] Problem 4:  Reinforcement learning\n",
    "\n",
    "Consider a **cube** state space defined by $0 \\le x, y, z \\le L$. Suppose you are piloting/programming a [drone](https://twitter.com/Reuters/status/981121633023705088/video/1) to learn how to land on a platform at the center of the $z=0$ surface (the bottom). Some assumptions:\n",
    "* In this discrete world, if I say the drone is at $(x,y,z)$ I mean that it is in the box centered at $(x,y,z)$. And there are boxes (states) centered at $(x,y,z)$ for all $0 \\le x,y,z \\le L$. Each state is a 1 unit cube. So when $L=2$ (for example), there are cubes centered at each $x=0,1,2$, $y=0,1,2$ and so on, for a total state space size of $3^3 = 27$ states.\n",
    "* All of the states with $z=0$ are terminal states.\n",
    "* The state at the center of the bottom of the cubic state space is the landing pad. So, for example, when $L=4$, the landing pad is at $(x,y,z) = (2,2,0)$.\n",
    "* All terminal states ***except*** the landing pad have a reward of -1. The landing pad has a reward of +1.\n",
    "* All non-terminal states have a reward of -0.01.\n",
    "* The drone takes up exactly 1 cubic unit, and begins each training episode in a random non-terminal state.\n",
    "* The available actions in non-terminal states include moving exactly 1 unit Up (+z), Down (-z), North (+y), South (-y), East (+x) or West (-x). In a terminal state, the training episode should end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part A\n",
    "Write a class `MDPLanding` to represent the Markov decision process for this drone. Include methods for:\n",
    "1. `actions(state)`, which should return a list of all actions available from the given state\n",
    "2. `reward(state)`, which should return the reward for the given state\n",
    "3. `result(state, action)`, which should return the resulting state of doing the given action in the given state\n",
    "\n",
    "and attributes for:\n",
    "1. `states`, which is just a list of all the states in the state space, where each state is represented as an $(x,y,z)$ tuple\n",
    "2. `terminal_states`, a dictionary where keys are the terminal state tuples and the values are the rewards associated with those terminal states\n",
    "3. `default_reward`, which is a scalar for the reward associated with non-terminal states\n",
    "4. `all_actions`, a list of all possible actions (Up, Down, North, South, East, West)\n",
    "5. `discount`, the discount factor (use $\\gamma = 0.999$ for this entire problem)\n",
    "\n",
    "How you feed arguments/information into the class constructor is up to you.\n",
    "\n",
    "Note that actions are *deterministic* here.  The drone does not need to learn transition probabilities for outcomes of particular actions. What the drone does need to learn, however, is where the heck that landing pad is, and how to get there from any initial state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Part B\n",
    "Write a function to implement **policy iteration** for this drone landing MDP. Create an MDP environment to represent the $L=4$ case (so 125 total states).\n",
    "\n",
    "Use your function to find an optimal policy for your new MDP environment. Check (by printing to screen) that the policy for the following states are what you expect, and comment on the results:\n",
    "1. $(2,2,1)$\n",
    "1. $(0,2,1)$\n",
    "1. $(2,0,1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part C\n",
    "\n",
    "Code up a **Q-learning** agent/algorithm to learn how to land the drone. You can do this however you like, as long as you use the MDP class structure defined above.  \n",
    "\n",
    "Your code should include some kind of a wrapper to run many trials to train the agent and learn the Q values (see Section 21.3.2 in the textbook).  You also do not need to have a separate function for the actual \"agent\"; your code can just be a \"for\" loop within which you are refining your estimate of the Q values.\n",
    "\n",
    "From each training trial, save the cumulative discounted reward (utility) over the course of that episode. That is, add up all of $\\gamma^t R(s_t)$ where the drone is in state $s_t$ during time step $t$, for the entire sequence. I refer to this as \"cumulative reward\" because we usually refer to \"utility\" as the utility *under an optimal policy*.\n",
    "\n",
    "Some guidelines:\n",
    "* The drone should initialize in a random non-terminal state for each new training episode.\n",
    "* The training episodes should be limited to 50 time steps, even if the drone has not yet landed. If the drone lands (in a terminal state), the training episode is over.\n",
    "* You may use whatever learning rate $\\alpha$ you decide is appropriate, and gives good results.\n",
    "* There are many forms of Q-learning. You can use whatever variety you would like.\n",
    "* Your code should return:\n",
    "  * The learned Q values associated with each state-action pair.\n",
    "  * The cumulative reward for each training trial. \n",
    "  * Anything else that might be useful in the ensuing analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part D\n",
    "\n",
    "Initialize the $L=10$ environment (so that the landing pad is at $(5,5,0)$). Run some number of training trials to train the drone.\n",
    "\n",
    "**How do I know if my drone is learned enough?**  If you take the mean cumulative reward across the last 5000 training trials, it should be around 0.80. This means at least about 10,000 (but probably more) training episodes will be necessary. It will take a few seconds on your computer, so start small to test your codes.\n",
    "\n",
    "**Then:** Compute block means of cumulative reward from all of your training trials. Use blocks of 500 training trials. This means you need to create some kind of array-like structure such that its first element is the mean of the first 500 trials' cumulative rewards; its second element is the mean of the 501-1000th trials' cumulative rewards; and so on. Make a plot of the block mean rewards as the training progresses. It should increase from about -0.5 initially to somewhere around +0.8.\n",
    "\n",
    "**And:** Print to the screen the mean of the last 5000 trials' cumulative rewards, to verify that it is indeed about 0.80."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part E\n",
    "\n",
    "**Question 1:** Why does the cumulative reward start off around -0.5 at the beginning of the training?\n",
    "\n",
    "**Question 2:** Why will it be difficult for us to train the drone to reliably obtain rewards much greater than about 0.8?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a/ id='bottom'></a>\n",
    "\n",
    "[Back to top](#top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
