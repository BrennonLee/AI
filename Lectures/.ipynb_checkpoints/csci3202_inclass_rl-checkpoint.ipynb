{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 3202 Spring 2018 - Friday 20 April\n",
    "\n",
    "# In-class notebook: Reinforcement learnin'\n",
    "\n",
    "<a id='top'></a>\n",
    "\n",
    "<br>\n",
    "\n",
    "* You will **not** submit this to Moodle. But you may find the implementation of a simple active reinforcement learning technique to be a useful starting point for a particular problem on the practicum.\n",
    "* Even though you may be developing some key codes in groups here, it is expected any codes you turn in on the practicum **are entirely your own**.  \n",
    "\n",
    "---\n",
    "\n",
    "Before we begin, let's load a few packages that we might find useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<img src='https://www.explainxkcd.com/wiki/images/9/9a/computers_vs_humans.png' style=\"width: 500px;\"/>\n",
    "\n",
    "## N-armed bandit\n",
    "\n",
    "As discussed in class, the N-armed bandit describes the following problem. There is a gambler (\"agent\") who has the option to play one of N slot machines. The agent can switch between machines, but of course would like to play the one with the highest probability of winning. Alas, these probabilities are unknown... ***for now!***\n",
    "\n",
    "### Learning the probabilities for each of the bandit's N arms\n",
    "\n",
    "Initialize a dictionary for keeping track of our best estimates of each lever's value, and the number of times we have pulled each lever.  We should also leave our code general to see how the number of options (levers to pull) affects how quickly we can learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "actions = list(range(N))\n",
    "Q = defaultdict(float)\n",
    "Nsa = defaultdict(int)\n",
    "Nwin = defaultdict(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we will need a policy for each state, which could be initialized as a random policy:\n",
    "```\n",
    "policy = {s : random action for s in states}\n",
    "```\n",
    "\n",
    "But here we just want something to return a random action.  Finish this function to pick a random action for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(actions):\n",
    "    '''Return a random action (uniform random!)'''\n",
    "    \n",
    "    # your code goes here...\n",
    "    return np.random.choice(actions)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to represent the result of the action of pulling a given lever.  So we need probabilities for each.  Let's use the ones from class for now, and write a function to return a win (+1) or loss (+0) with those probabilities, based on which lever we pull (`action`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_levers = [0.3, 0.5, 0.8]\n",
    "\n",
    "def result(action, p_levers):\n",
    "    ''' Return a win (+1) with probability p_levers for the \n",
    "    lever pulled, and a loss (+0) with probability 1-p_levers[action].\n",
    "    action = which lever you pull (index within p_levers)\n",
    "    '''\n",
    "    \n",
    "    # your code goes here...\n",
    "    return np.random.choice([1,0], p=[p_levers[action], 1-p_levers[action]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First, consider the following:**  What is the best you could hope to do, on average, if you *knew* what the probabilities of winning are on any given slot machine?  That is, what is you knew `p_levers`?\n",
    "\n",
    "<br>\n",
    "\n",
    "### Training\n",
    "\n",
    "Let's run 10 training episodes and see what we get. First, set the random seed so at least for the first step here, we will all be on the same page.\n",
    "\n",
    "For now, save the results of each training episode in a list, along with the reward from each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 2, 2, 1, 2, 2, 2, 2, 1]\n",
      "[1, 1, 1, 1, 0, 1, 1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(3202)\n",
    "\n",
    "levers = []\n",
    "rewards = []\n",
    "for k in range(10):\n",
    "    # your code goes here!   # pick a lever to pull\n",
    "    levers.append(random_policy(actions))\n",
    "    # your code goes here!   # what is the result of that pull?\n",
    "    rewards.append(result(levers[k], p_levers))\n",
    "    \n",
    "print(levers)\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The important question:** What are our estimates of the expected utility for each lever?  That is, what are our best estimates of $Q(0)$, $Q(1)$ and $Q(2)$ after this brief training period?\n",
    "\n",
    "Note that we can obtain the total number of times lever 0 was pulled as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nsa[0] = sum([l==0 for l in levers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can get the number of times lever 0 resulted in a win like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nwin[0] = sum([l==0 and r==1 for l,r in zip(levers,rewards)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which makes our best guess for the expected utility of lever 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q[0] = Nwin[0]/Nsa[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the counters `Nwin` and `Nsa`, and calculate our best estimates of `Q` for levers 1 and 2 as well.  How close are these to the actual values?  Should we continue to **explore** or start **exploiting** at this point?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q0 1.0\n",
      "Q1 0.5\n",
      "Q2 0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "Nsa[1] = sum([l==1 for l in levers])\n",
    "Nwin[1] = sum([l==1 and r==1 for l,r in zip(levers,rewards)])\n",
    "Q[1] = Nwin[1]/Nsa[1]\n",
    "\n",
    "Nsa[2] = sum([l==2 for l in levers])\n",
    "Nwin[2] = sum([l==2 and r==1 for l,r in zip(levers,rewards)])\n",
    "Q[2] = Nwin[2]/Nsa[2]\n",
    "\n",
    "print('Q0',Q[0])\n",
    "print('Q1',Q[1])\n",
    "print('Q2',Q[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's actively learn!\n",
    "\n",
    "First, we need to include an online estimate of `Q` for each state, that updates `Nsa`, `Nwin` and `Q` as the training progresses, as opposed to after the fact. That is, we need to calculate these things **within** the 'for' loop as opposed to outside of it.  Make sure that you again set the random seed the same as the first time you did this training, so you can check that the results are the same with your new online updating of `Q`.\n",
    "\n",
    "**Potentially useful fact:**  in Python, `x[-1]` returns the *last* item of a list `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'float'>, {0: 1.0, 2: 0.7142857142857143, 1: 0.5})\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(3202)\n",
    "\n",
    "Q = defaultdict(float)       # re-initializing each time we train\n",
    "Nsa = defaultdict(int)\n",
    "Nwin = defaultdict(int)\n",
    "levers = []\n",
    "rewards = []\n",
    "\n",
    "for k in range(10):\n",
    "    # your code goes here!   # pick a lever to pull\n",
    "    levers.append(random_policy(actions))\n",
    "    # your code goes here!   # what is the result of that pull?\n",
    "    rewards.append(result(levers[-1], p_levers))\n",
    "    # your code goes here!   # update Nsa\n",
    "    Nsa[levers[-1]] = sum([l==levers[-1] for l in levers])\n",
    "\n",
    "\n",
    "    # your code goes here!   # update Nwin\n",
    "    Nwin[levers[-1]] = sum([l==levers[-1] and r==1 for l,r in zip(levers,rewards)])\n",
    "    # your code goes here!   # update Q\n",
    "    Q[levers[-1]] = Nwin[levers[-1]]/Nsa[levers[-1]]\n",
    "    \n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subroutinization for training sampling\n",
    "\n",
    "Before we go any further, we really ought to subroutinize that training loop above.  We also should add some measure of how well we are doing.  So we can (1) track the rewards we've gotten, and (2) use the mean of the last 100 rewards as a measure of how good our strategy is. And, after the 100th training episode, we can keep track of this measure as the training progresses; if it increases, then we are learning! And that's nice.\n",
    "\n",
    "The inputs to our training function are the number of training episodes to run, the available `actions` and `p_levers`, and the outputs should be the counters `Nsa` and `Nwin`, the latest utility estimates `Q` and our time series of mean rewards, `avg_reward`. Finish off this function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(n_train, actions, p_levers):\n",
    "    \n",
    "    assert n_train >= 100, 'Need at least 100 training episodes'\n",
    "    \n",
    "    Q = defaultdict(float)       # re-initializing\n",
    "    Nsa = defaultdict(int)\n",
    "    Nwin = defaultdict(int)\n",
    "    levers = []                  # not actually necessary to keep track of\n",
    "    rewards = []                 # to calculate running average reward\n",
    "    avg_reward = []              # running average reward\n",
    "    \n",
    "    for k in range(n_train):\n",
    "        # your code goes here!   # pick a lever to pull\n",
    "        levers.append(random_policy(actions))\n",
    "        # your code goes here!   # what is the result of that pull?\n",
    "        rewards.append(result(levers[-1], p_levers))\n",
    "        # your code goes here!   # update Nsa\n",
    "        Nsa[levers[-1]] = sum([l==levers[-1] for l in levers])\n",
    "\n",
    "\n",
    "        # your code goes here!   # update Nwin\n",
    "        Nwin[levers[-1]] = sum([l==levers[-1] and r==1 for l,r in zip(levers,rewards)])\n",
    "        # your code goes here!   # update Q\n",
    "        Q[levers[-1]] = Nwin[levers[-1]]/Nsa[levers[-1]]\n",
    "\n",
    "        # If we are beyond the 100th training episode,\n",
    "        # then update the running average reward\n",
    "        if k >= 100:\n",
    "            avg_reward.append(np.mean(rewards[-100:]))\n",
    "\n",
    "    return Nsa, Nwin, Q, avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-4caae4580e76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mNsa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNwin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_levers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-83-af05a73a221d>\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(n_train, actions, p_levers)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# your code goes here!   # update Nwin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mNwin\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mlevers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;31m# your code goes here!   # update Q\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNwin\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mNsa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_train = 10000\n",
    "Nsa, Nwin, Q, avg_reward = run_training(n_train, actions, p_levers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A question of interest:** What is the expected reward of any given trial?\n",
    "\n",
    "**[A]** You can calculate this using the Law of Total Probability or estimate by running the above training simulation for many iterations and making an estimate of the probability of winning, regardless of lever pulled. Do that now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Expected reward is: {}'.format(sum(Nwin.values())/n_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[B]** Let's also make a plot of the running average for mean reward as the training progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = list(range(100,100+len(avg_reward)))  # episodes we have output for\n",
    "plt.plot(episodes, avg_reward)\n",
    "plt.xlabel('Training episode')\n",
    "plt.ylabel('Running mean reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consider**:  how should the number from **A** and the plot from **B** be related?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\epsilon$-greedy agent\n",
    "\n",
    "So far, the agent playing this slot machine game has not been trying to *improve* their strategy based on the estimates of `Q`.  So let's fix that.\n",
    "\n",
    "Let's implement an $\\epsilon$-greedy agent. Below is a function to for selecting an $\\epsilon$-greedy action, with a default $\\epsilon$ (probability of acting randomly) of 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_action(actions, Q, epsilon=0.1):\n",
    "    '''Return a random action or the one with highest utility (so far).\n",
    "    Also will return a random action if we have not observed anything\n",
    "    to estimate Q yet.\n",
    "    '''\n",
    "    if (np.random.uniform()<=epsilon) or (len(Q)==0):\n",
    "        action = random_policy(actions)\n",
    "    else:\n",
    "        action = max([(p,a) for (a, p) in Q.items()])[1]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revise the `run_training` routine ever so slightly so that instead of getting a random action for which lever to pull, the agent chooses an `epsilon_greedy_action`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_smart_training(n_train, actions, p_levers):\n",
    "    \n",
    "    # your code goes here...\n",
    "    \n",
    "    return Nsa, Nwin, Q, avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now re-run the training with an $\\epsilon$-greedy agent using the constant $\\epsilon = 0.1$. Do 10,000 training episodes and report the expected reward. Generate another plot showing the running average reward as training progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training\n",
    "n_train = 10000\n",
    "Nsa, Nwin, Q, avg_reward = run_smart_training(n_train, actions, p_levers)\n",
    "\n",
    "# Print the expected reward\n",
    "print('Expected reward is: {}'.format(sum(Nwin.values())/n_train))\n",
    "\n",
    "# Make a plot of the running average reward\n",
    "episodes = list(range(100,100+len(avg_reward)))  # episodes we have output for\n",
    "plt.plot(episodes, avg_reward)\n",
    "plt.xlabel('Training episode')\n",
    "plt.ylabel('Running mean reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some questions:**\n",
    "\n",
    "1. How well does our $\\epsilon$-greedy agent do?\n",
    "1. How well theoretically *could* our agent do, if it knew exactly what the probabilities of each lever paying out are?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\epsilon$-decreasing greedy agent\n",
    "\n",
    "Let's see if we can get closer to that theoretical upper limit, where the agent figures out quickly which lever is the best and starts pulling that one sooner.\n",
    "\n",
    "To do this, we can implement an $\\epsilon$-decreasing agent, where we pick some functional form for $\\epsilon$ that decreases as the training progresses.  You may want to play around with different forms of this function, but this fairly simple problem does not require anything too fancy.\n",
    "\n",
    "We can implement this $\\epsilon$-decreasing within the `run_smart_training` function by modifying the `epsilon` argument that is fed into `epsilon_greedy_action`.  Call this version of your training routine `run_smarter_training`, and make the code modifications below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_smarter_training(n_train, actions, p_levers):\n",
    "    \n",
    "    # your code goes here...\n",
    "\n",
    "    return Nsa, Nwin, Q, avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now train this new and improved agent!  How well can the $\\epsilon$-decreasing agent do, relative to our theoretical upper limit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run the training\n",
    "n_train = 10000\n",
    "Nsa, Nwin, Q, avg_reward = run_smarter_training(n_train, actions, p_levers)\n",
    "\n",
    "# Print the expected reward\n",
    "print('Expected reward is: {}'.format(sum(Nwin.values())/n_train))\n",
    "\n",
    "# Make a plot of the running average reward\n",
    "episodes = list(range(100,100+len(avg_reward)))  # episodes we have output for\n",
    "plt.plot(episodes, avg_reward)\n",
    "plt.xlabel('Training episode')\n",
    "plt.ylabel('Running mean reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other ways to make things more complicated\n",
    "\n",
    "If you've made it this far, there are a couple extra ways in which we can spice things up and make the problem more difficult.\n",
    "\n",
    "1. Try using more levers.\n",
    "1. Try incentivizing the agent to explore all of the levers a set number of times (i.e., implement an **exploration function**).\n",
    "1. Try making it tougher for the agent to tell which lever is better than any others (i.e., make the `p_levers` values closer together)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
