{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 3202, Spring 2018\n",
    "\n",
    "# Friday 16 March 2018\n",
    "\n",
    "# In-class notebook:  Bayesian networks\n",
    "\n",
    "<a id='top'></a>\n",
    "\n",
    "<br>\n",
    "\n",
    "### Your name(s):\n",
    "\n",
    "<br>\n",
    "\n",
    "You **are not** submitting this to Moodle as a Quizlet, but it could be useful for a future Quizlet.\n",
    "\n",
    "---\n",
    "\n",
    "Shortcuts:  [Top](#top) || [BayesNode](#node) | [Probabilities](#probs) | [Sidewalk](#sidewalk) | [Approx. Inference](#approx) | [Cond. Probs.](#conditional) || [Conclusions](#conclusions)\n",
    "\n",
    "---\n",
    "\n",
    "Before we begin, let's load a few packages that we might find useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='node'></a>\n",
    "### Class for nodes in a Bayesian network\n",
    "\n",
    "Below defines a class `BayesNode`, which will store everything we need for a discrete node state in a Bayesian network (Bayes net).\n",
    "\n",
    "The constructor requires 3 arguments:\n",
    "* `name` is a string for the variable name\n",
    "* `parents` is a list of strings representing the names of the parent variables\n",
    "* `cpt` is a dictionary where the keys are tuples representing parent state values and the dict values are the resulting probabilities of this variable.\n",
    "  * If this variable has no parents, then the key should be an empty tuple.\n",
    "  * And recall that it is only necessary to store the probability of `True` for a Boolean variable. In general, we must store the number of possible values minus 1 for any discrete variable state.\n",
    "  \n",
    "And if you are curious, that `__repr__` method is defined so that Python returns some reasonable-looking output when we print a `BayesNode` object to screen. (more on this later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesNode:\n",
    "    \n",
    "    def __init__(self, name, parents, cpt):\n",
    "        if isinstance(parents, str):\n",
    "            parents = parents.split()\n",
    "            \n",
    "        if len(parents)==0:\n",
    "            # if no parents, empty tuple dict key for cpt\n",
    "            cpt = {(): cpt}\n",
    "        elif isinstance(cpt, dict):\n",
    "            # if there is only one parent, only one tuple argument\n",
    "            if cpt and isinstance(list(cpt.keys())[0], bool):\n",
    "                cpt = {(v): p for v, p in cpt.items()}\n",
    "\n",
    "        self.variable = name\n",
    "        self.parents = parents\n",
    "        self.cpt = cpt\n",
    "        self.children = []\n",
    "        \n",
    "#     def __repr__(self):\n",
    "#         return repr((self.variable, ' '.join(self.parents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are only dealing with Boolean variables in this exercise, for brevity's sake we can define `T` and `F` to represent our typical truth values.  But note that here and in other Boolean Bayes net applications, we will frequently use $+x$ to denote the event $X=\\text{True}$ and $-x$ for the event $X=\\text{False}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = True\n",
    "F = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='probs'></a>\n",
    "### Calculating probabilities from the `BayesNode` objects\n",
    "\n",
    "Here is a helpful function to calculate the probability of seeing Boolean `BayesNode` variable `var==value`, when the parents' values are given in `evidence`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P(var, value, evidence={}):\n",
    "    '''The probability distribution for P(var | evidence), \n",
    "    when all parent variables are known (in evidence) and\n",
    "    we have Boolean variables'''\n",
    "    if len(var.parents)==1:\n",
    "        # only one parent\n",
    "        row = evidence[var.parents[0]]\n",
    "    else:\n",
    "        # multiple parents\n",
    "        row = tuple(evidence[parent] for parent in var.parents)\n",
    "    return var.cpt[row] if value else 1-var.cpt[row]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this machinery to calculate a probability from a hopefully familiar Bayesian network. Consider the Bayes net for falling on the sidewalk outside your house, from the Quizlet 9:\n",
    "\n",
    "<img src=\"http://www.cs.colorado.edu/~tonyewong/home/resources/bayesnet_sidewalk.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "Use the `BayesNode` class to create node objects for each of `Shoes`, `Icy` and `Fall`. The first one is done for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "Shoes = BayesNode('Shoes', '', 0.5)\n",
    "Icy = BayesNode('Icy', '', 0.1)\n",
    "Fall = BayesNode('Fall', 'Shoes Icy', {(T,T): 0.4, (T,F): 0.7,(F,T): 0.1, (F,F): 0.01 })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### A short detour:  printable representations in Python\n",
    "\n",
    "If you are not sure what exactly that `__repr__` method is doing, let's find out!  Try printing to the screen the `Shoes` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.BayesNode object at 0x1151b0048>\n"
     ]
    }
   ],
   "source": [
    "print(Shoes.__repr__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, go back to the `BayesNode` class and comment out the `__repr__` method.  Then, redefine your `Shoes` object using the slightly revised `BayesNode` class.  Finally, let's see how this affects the output when we print `Shoes`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.BayesNode object at 0x1151b0048>\n"
     ]
    }
   ],
   "source": [
    "Shoes = BayesNode('Shoes', '', 0.5)\n",
    "print(Shoes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLECH!** That's what we like the modifiable representation of our objects. Go change it back and redefine the `BayesNode` class and the `Shoes` node!\n",
    "\n",
    "**The point:**  You can add and/or modify that `__repr__` method for any class you define, which gives some nice flexibility in interpreting output, and potentially for debugging.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sidewalk'></a>\n",
    "### Back to our jokester roommate and icy sidewalk\n",
    "\n",
    "Check a few probabilities to make sure you know how the `P` function works.  One example is given as an `assert` statement below, but it's worthwhile to check some of the other ones, including checking if a variable is `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert P(Fall, T, {'Shoes' : F, 'Icy' : T})==0.1, \"Something has gone wrong!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the function `P` to calculate the probability of falling down on any given day as you leave your house.\n",
    "\n",
    "As you saw on the Quizlet,\n",
    "\n",
    "$$\\begin{align*}\n",
    "  P(F) &= \\sum_{s,c} P(F \\mid s, c) P(s,c) \\\\\n",
    "       &= \\sum_s \\sum_c P(F \\mid s,c) P(s) P(c) \\\\\n",
    "       &= \\sum_s P(s) \\sum_c P(F \\mid s,c) P(c) \\\\\n",
    "  \\end{align*}$$\n",
    "  \n",
    "Check this calculation using the nodes you defined above and the `P` function. Recall that $P(+f) = 0.3445$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='approx'></a>\n",
    "# Approximate inference\n",
    "\n",
    "The algorithm:\n",
    "\n",
    "1. For each $X_i$ in $X_1, X_2, \\ldots , X_{n-1}, X_n$:\n",
    "  1. Draw a sample from $P(X_i \\mid \\text{parents}(X_i))$ (it's nice to be vectorized, if possible)\n",
    "  1. If $X_i$ has no parents, then draw from the **prior** distribution, $P(X_i)$\n",
    "1. Draw large samples for each variable all at once, or do this many, many times.\n",
    "  \n",
    "Let's suppose we have a node ordering of `Shoes`, `Icy`, `Fall`.\n",
    "\n",
    "The first step is then to draw a sample from $P(\\text{Shoes})$.  Let's make a sample of size 10000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 10000\n",
    "sample_shoes = np.random.choice([T,F], size=n_sample, p=[P(Shoes, T), 1-P(Shoes, T)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with a bit of foresight, we realize that we will quickly have a lot of arrays and lists flying around with variable-specific names!  Especially when both `sample_icy` and `sample_shoes` are going to be pretty much the same thing, which is `n_sample` random draws from some distribution.\n",
    "\n",
    "So instead of having a bunch of separate arrays, we can store all these samples for our approximate Bayesian computation in a Pandas DataFrame.  We can convert our `sample_shoes` array into a DataFrame using the following command.  It will place our array `sample_shoes` as a column in the DataFrame with the title we give it.  \"Shoes\" seems like a reasonable title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSample = pd.DataFrame({'Shoes' : sample_shoes})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step: we move on to the next varable, `Icy`.  We draw a sample of `n_sample` realizations from this variable's distribution too, conditioned on the previous variables.  But the two are independent, so we are sampling from the prior $P(\\text{Icy})$, similar to our sample for `Shoes`.  We can store this straightaway as a new column of our DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSample['Icy'] = np.random.choice([T,F], size=n_sample, p=[P(Icy, T), 1-P(Icy, T)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't used Pandas DataFrames before, you might be wondering why the heck we're going through this trouble. Turns out they have some nice features. For example, you can calculate the mean of each column quickly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Shoes    0.5039\n",
       "Icy      0.0976\n",
       "dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfSample.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or plot histograms of the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115079908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfSample.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Do the results for the mean and histograms of our `Shoes` and `Icy` samples match what we expect from their prior distributions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we need to sample from the conditional distribution of `Fall`, given the values of `Shoes` and `Icy` that we sampled. We put our previous samples into a DataFrame to make it easier to keep track of which samples from the priors (causes) lead to which samples from the effects variables (like `Fall`).\n",
    "\n",
    "Each row in `dfSample` corresponds to a different sample from the prior distributions of `Shoes` and `Icy`. So we need to `apply` to each row of `dfSample` a function to draw a sample for `Fall` from the conditional distribution $P(\\text{Fall} \\mid \\text{Shoes, Icy})$, where the values for `Shoes` and `Icy` are brought in from the first two columns of `dfSample`.\n",
    "\n",
    "Define a function `sample_2parent(row, variable, parents)` that returns a random sample from the conditional distribution of `variable`, given the values of its `parents` in the given `row` of `dfSample`.\n",
    "* `row`, a given row of the DataFrame `dfSample`. This will include the column headers, which are useful for referencing elements.\n",
    "* `variable`, a `BayesNode` object whose conditional probability we want to sample from\n",
    "* `parents`, a list of variable names (strings) that give the parents of the variable we want to sample. As the name suggests, you may assume there are two parents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_2parent(row, variable, parents):\n",
    "\n",
    "    # your code goes here!\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the Pandas `apply` method to apply this function to each row of our sample DataFrame.  This will make a draw from the 2-parent conditional distribution of `variable=Fall`, given the values of its `parents`, for each of the rows in `dfSample`.  And we will store the result in a new column of `dfSample`, called `Fall`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfSample['Fall'] = dfSample.apply(sample_2parent, axis=1, variable=Fall, parents=Fall.parents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what is the marginal probability of falling outside our house, $P(\\text{Fall})$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSample['Fall'].sum()/n_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Does that match what you expect based on the Quizlet calculation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is what we call **prior sampling**.  That is, we draw from the variables' prior distributions, and use that information to \"trickle down\" our sampling through the Bayes net. These probabilities (samples) propagate by using the conditional probabilities that link from causes to effects, the way the samples for `Fall` were conditioned on our previous samples for `Shoes` and `Icy`.\n",
    "\n",
    "We can evaluate **joint probabilities** of events by looking at how many samples satisfy those events, out of the total number of samples.\n",
    "\n",
    "**Question:** For example, what is the joint probability of your shoes were tied together, your sidewalk was not icy, and you did not fall down?  That is, what is our estimate of $P(+s, -i, -f)$?\n",
    "\n",
    "Below is a snippet of code to extract only the rows of `dfSample` that match these criteria.  You will need to modify this in order to estimate $P(+s, -i, -f)$. You should get about 0.1343."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfSample.loc[(dfSample['Shoes']==True) & \n",
    "             (dfSample['Icy']==False)  & \n",
    "             (dfSample['Fall']==False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='conditional'></a>\n",
    "### Estimating conditional probabilities\n",
    "\n",
    "Now the whole point was to sample from conditional distributions of things we didn't observe, *given* the values of evidence that we can observe.\n",
    "\n",
    "So let's estimate: what is the probability that our roommate tied our shoes together, given that our walkway was not icy and we did not fall down? That is, what is our estimate of $P(+s \\mid -f, -i)$?\n",
    "\n",
    "We can estimate this using what the book calls **rejection sampling** (see my editorial comments below). It is precisely what we did in the Socks example!  We have a large sample from the prior distribution, and we want to ***reject*** any samples that do not agree with our data.  In this case, our data is `Fall=False` and `Icy=False`.\n",
    "\n",
    "We have two options:\n",
    "\n",
    "**One way** that we could estimate $P(+s \\mid -f, -i)$ is to use the definition of conditional probability to rewrite this as:\n",
    "$$P(+s \\mid -f, -i) = \\dfrac{P(+s, -f, -i)}{P(-f, -i)}$$\n",
    "Then we can estimate $P(+s \\mid -f, -i)$ as the ratio of the number of rows satisfying all three conditions from the numerator simultaneously (`+s`, `-f`, and `-i`), and the number of rows satisfying both conditions of the denominator simulataneously (`-f` and `-i`, with no regard for the value of `S`).\n",
    "\n",
    "**Another way** that we could estimate $P(+s \\mid -f, -i)$ is to look at only the rows where our conditions (`-f` and `-i`) are satisfied, and report the proportion of those rows in which our query (`+s`) is `True`.\n",
    "\n",
    "Calculate an estimate for $P(+s \\mid -f, -i)$ using one (or better yet, both!) of these methods.  On Monday, we will finish off exact inference for Bayesian networks, and will work out this probability by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Consider our sample size.  How accurately can we reasonably expect to estimate these posterior probabilities with a sample of size $N$?  Argue with your friends starting... *now*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='conclusions'></a>\n",
    "### Conclusions\n",
    "\n",
    "The process of generating all those samples is what the book refers to as **prior sampling**.\n",
    "\n",
    "Once we rejected all of the samples that did not match our data (`Fall` and `Icy` both `False`), *that* was what the books refers to as \"rejection sampling\".  It is the same as what we did in the Sock Example!  Technically, this is rejection sampling... but I would much rather call it **approximate inference**.  Rejection sampling refers to a wider class of sampling methods, much more powerful than this simple method.\n",
    "\n",
    "**Coding practices:**  Was it bad practice to take up several single-letter variable names with silly stuff like `T=True` and a function for probabilities hogging up the `P` variable?  Maybe.  But we probably want to have longer names for the variables that actually mean something. Plus it would have been super annoying to keep typing out `True` or `Probability`.  I don't have the answer, but it is something to keep in mind (especially as you debug)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "[Back to top](#top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
